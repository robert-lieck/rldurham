
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/Lecture_4_Dynamic_Programming.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_Lecture_4_Dynamic_Programming.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_Lecture_4_Dynamic_Programming.py:


Lecture 4: Dynamic Programming
==============================

.. GENERATED FROM PYTHON SOURCE LINES 6-9

.. code-block:: Python


    # # Lecture 4: Dynamic Programming








.. GENERATED FROM PYTHON SOURCE LINES 10-17

.. code-block:: Python



    import numpy as np
    from scipy.special import softmax
    import rldurham as rld









.. GENERATED FROM PYTHON SOURCE LINES 18-35

.. code-block:: Python



    env = rld.make(
        'FrozenLake-v1',         # import the frozen lake environment
        # 'FrozenLake8x8-v1',    # use a bigger version
        render_mode="rgb_array", # for rendering as image/video
        is_slippery=False,       # warning: slippery=True results in very complex environment dynamics where the optimal solution is not very intuitive to humans!
        # desc=["GFFS", "FHFH", "FFFH", "HFFG"],  # define custom map
    )
    rld.seed_everything(42, env)
    # some info
    rld.env_info(env, print_out=True)
    print('action space: ' + str(env.action_space))
    print('observation space: ' + str(env.observation_space))
    rld.render(env)





.. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_001.png
   :alt: Lecture 4 Dynamic Programming
   :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    actions are discrete with 4 dimensions/#actions
    observations are discrete with 16 dimensions/#observations
    maximum timesteps is: 100
    action space: Discrete(4)
    observation space: Discrete(16)




.. GENERATED FROM PYTHON SOURCE LINES 36-53

.. code-block:: Python



    # actions
    LEFT, DOWN, RIGHT, UP = 0,1,2,3

    # lets do an example step for the policy
    env.reset()
    next_state, reward, term, trunc, info = env.step(RIGHT)
    print('=============')
    print('next state: ' + str(next_state))
    print('terminated: ' + str(term))
    print('truncated: ' + str(trunc))
    print('    reward: ' + str(reward))
    print('      info: ' + str(info))
    rld.render(env, clear=False)





.. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_002.png
   :alt: Lecture 4 Dynamic Programming
   :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    =============
    next state: 1
    terminated: False
    truncated: False
        reward: 0.0
          info: {'prob': 1.0}




.. GENERATED FROM PYTHON SOURCE LINES 54-58

Policy evaluation
-----------------



.. GENERATED FROM PYTHON SOURCE LINES 60-80

.. code-block:: Python



    def policy_evaluation(env, policy, gamma=1, theta=1e-8, draw=False):
        V = np.zeros(env.observation_space.n)
        while True:
            delta = 0
            for s in range(env.observation_space.n):
                Vs = 0
                for a, action_prob in enumerate(policy[s]):
                    for prob, next_state, reward, done in env.P[s][a]:
                        Vs += action_prob * prob * (reward + gamma * V[next_state])
                delta = max(delta, np.abs(V[s]-Vs))
                V[s] = Vs
            if draw:
                rld.plot_frozenlake(env=env, v=V, policy=policy, draw_vals=True)
            if delta < theta:
                break
        return V









.. GENERATED FROM PYTHON SOURCE LINES 81-87

.. code-block:: Python



    # lets start with a random policy, in this case there's a 1/4 probability of taking any action at every 4x4 state
    policy = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n









.. GENERATED FROM PYTHON SOURCE LINES 88-96

.. code-block:: Python



    # evaluate this policy (change draw=True to show steps, and ensure environment is 'FrozenLake-v1' for the exact same steps in the lecture)
    V = policy_evaluation(env, policy, draw=True)


    # Get $q_\pi$ form $v_\pi$ by a one-step look ahead




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_003.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_003.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_004.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_004.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_005.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_005.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_006.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_006.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_007.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_007.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_008.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_008.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_009.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_009.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_010.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_010.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_011.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_011.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_012.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_012.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_013.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_013.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_014.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_014.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_015.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_015.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_016.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_016.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_017.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_017.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_018.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_018.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_019.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_019.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_020.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_020.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_021.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_021.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_022.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_022.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_023.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_023.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_024.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_024.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_025.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_025.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_026.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_026.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_027.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_027.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_028.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_028.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_029.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_029.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_030.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_030.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_031.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_031.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_032.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_032.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_033.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_033.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_034.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_034.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_035.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_035.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_036.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_036.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_037.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_037.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_038.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_038.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_039.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_039.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_040.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_040.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_041.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_041.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_042.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_042.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_043.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_043.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_044.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_044.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_045.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_045.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_046.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_046.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_047.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_047.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_048.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_048.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_049.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_049.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_050.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_050.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_051.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_051.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_052.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_052.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_053.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_053.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_054.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_054.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_055.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_055.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_056.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_056.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_057.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_057.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_058.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_058.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_059.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_059.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/rldurham/rldurham/rldurham/__init__.py:81: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
      plt.figure(figsize=(3, 3))




.. GENERATED FROM PYTHON SOURCE LINES 97-107

.. code-block:: Python



    def q_from_v(env, V, s, gamma=1):
        q = np.zeros(env.action_space.n)
        for a in range(env.action_space.n):
            for prob, next_state, reward, done in env.P[s][a]:
                q[a] += prob * (reward + gamma * V[next_state])
        return q









.. GENERATED FROM PYTHON SOURCE LINES 108-128

.. code-block:: Python



    def policy_improvement(env, V, gamma=1):
        policy = np.zeros([env.observation_space.n, env.action_space.n]) / env.action_space.n
        for s in range(env.observation_space.n):
            q = q_from_v(env, V, s, gamma)

            # # deterministic policy (will always choose one specific an action and does not capture the distribution)
            # policy[s][np.argmax(q)] = 1

            # stochastic optimal policy (puts equal probability on all maximizing actions)
            best_a = np.argwhere(q==np.max(q)).flatten()
            policy[s] = np.sum([np.eye(env.action_space.n)[i] for i in best_a], axis=0) / len(best_a)

            # # softmax policy that adds some exploration
            # policy[s] = softmax(q / 0.01)

        return policy









.. GENERATED FROM PYTHON SOURCE LINES 129-137

.. code-block:: Python



    # plot the policy from a single greedy improvement step after policy evaluation
    policy = policy_improvement(env, V)
    rld.plot_frozenlake(env, V, policy, draw_vals=True)
    rld.plot_frozenlake(env, V, policy, draw_vals=False)





.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_060.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_060.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_061.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_061.png
         :class: sphx-glr-multi-img





.. GENERATED FROM PYTHON SOURCE LINES 138-147

.. code-block:: Python



    # lets from here on use a larger grid world
    env = rld.make('FrozenLake8x8-v1', is_slippery=False)
    rld.seed_everything(42, env)
    policy = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n
    V = policy_evaluation(env, policy, draw=False)









.. GENERATED FROM PYTHON SOURCE LINES 148-155

.. code-block:: Python



    # show improved policy from the policy evaluation, for the 8x8 case it's still not great
    new_policy = policy_improvement(env, V)
    rld.plot_frozenlake(env, V, new_policy)





.. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_062.png
   :alt: Lecture 4 Dynamic Programming
   :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_062.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 156-176

.. code-block:: Python



    # now solve the MDP by policy iteration
    def policy_iteration(env, gamma=1, theta=1e-8):
        policy = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n
        while True:

            # evaluate the policy (get the value function)
            V = policy_evaluation(env, policy, gamma, theta)

            # greedily choose the best action
            new_policy = policy_improvement(env, V)

            if np.max(abs(policy_evaluation(env, policy) - policy_evaluation(env, new_policy))) < theta*1e2:
               break;

            policy = new_policy.copy()
        return policy, V









.. GENERATED FROM PYTHON SOURCE LINES 177-185

.. code-block:: Python



    # do policy iteration
    policy_pi, V_pi = policy_iteration(env, gamma=0.7)
    rld.plot_frozenlake(env, V_pi, policy_pi, draw_vals=True)
    rld.plot_frozenlake(env, V_pi, policy_pi, draw_vals=False)





.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_063.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_063.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_064.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_064.png
         :class: sphx-glr-multi-img





.. GENERATED FROM PYTHON SOURCE LINES 186-205

.. code-block:: Python



    # now lets do value iteration, which is the k=1 case but simplifies saving computation
    # note how there are no intermediate policies until the end
    def value_iteration(env, gamma=1, theta=1e-8):
        V = np.zeros(env.observation_space.n) # initial state value function
        while True:
            delta = 0
            for s in range(env.observation_space.n):
                v_s = V[s] # store old value
                q_s = q_from_v(env, V, s, gamma) # the action value function is calculated for all actions
                V[s] = max(q_s) # the next value of the state function is the maximum of all action values
                delta = max(delta, abs(V[s] - v_s))
            if delta < theta: break
        # lastly, at convergence, we can get a (optimal) policy from the optimal state value function
        policy = policy_improvement(env, V, gamma)
        return policy, V









.. GENERATED FROM PYTHON SOURCE LINES 206-213

.. code-block:: Python



    policy_pi, V_pi = value_iteration(env, gamma=0.7)
    rld.plot_frozenlake(env, V_pi, policy_pi, draw_vals=True)
    rld.plot_frozenlake(env, V_pi, policy_pi, draw_vals=False)





.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_065.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_065.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_066.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_066.png
         :class: sphx-glr-multi-img





.. GENERATED FROM PYTHON SOURCE LINES 214-238

.. code-block:: Python



    # here's an expanded version of the above that's more similar to lecture slides
    def value_iteration(env, gamma=1, theta=1e-8):
        V = np.zeros(env.observation_space.n)
        while True:
            delta = 0
            for s in range(env.observation_space.n):
                v_s = V[s]

                # one step look ahead to get q from v
                q_s = np.zeros(env.action_space.n)
                for a in range(env.action_space.n):
                    for prob, next_state, reward, done in env.P[s][a]:
                        q_s[a] += prob * (reward + gamma * V[next_state])

                V[s] = max(q_s)
                delta = max(delta, abs(V[s] - v_s))
            if delta < theta: break

        policy = policy_improvement(env, V, gamma)
        return policy, V









.. GENERATED FROM PYTHON SOURCE LINES 239-245

.. code-block:: Python



    policy_pi, V_pi = value_iteration(env, gamma=0.7)
    rld.plot_frozenlake(env, V_pi, policy_pi, draw_vals=True)
    rld.plot_frozenlake(env, V_pi, policy_pi, draw_vals=False)




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_067.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_067.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_068.png
         :alt: Lecture 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Lecture_4_Dynamic_Programming_068.png
         :class: sphx-glr-multi-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 8.602 seconds)


.. _sphx_glr_download_auto_examples_Lecture_4_Dynamic_Programming.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: Lecture_4_Dynamic_Programming.ipynb <Lecture_4_Dynamic_Programming.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: Lecture_4_Dynamic_Programming.py <Lecture_4_Dynamic_Programming.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: Lecture_4_Dynamic_Programming.zip <Lecture_4_Dynamic_Programming.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
