
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/Practical_4_Dynamic_Programming.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_Practical_4_Dynamic_Programming.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_Practical_4_Dynamic_Programming.py:


Practical 4: DynamicProgramming
===============================

.. GENERATED FROM PYTHON SOURCE LINES 6-9

.. code-block:: Python


    # # Practical 4: DynamicProgramming








.. GENERATED FROM PYTHON SOURCE LINES 10-16

.. code-block:: Python



    import numpy as np
    import rldurham as rld









.. GENERATED FROM PYTHON SOURCE LINES 17-26

.. code-block:: Python



    name = 'FrozenLake-v1'  # small version
    # name = 'FrozenLake8x8-v1'  # larger version
    env = rld.make(name, is_slippery=False)
    rld.seed_everything(42, env)
    LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3









.. GENERATED FROM PYTHON SOURCE LINES 27-35

.. code-block:: Python



    print('action space: ' + str(env.action_space))
    print('reward range: ' + str(env.reward_range))
    print('observation space: ' + str(env.observation_space))
    rld.plot_frozenlake(env=env)





.. image-sg:: /auto_examples/images/sphx_glr_Practical_4_Dynamic_Programming_001.png
   :alt: Practical 4 Dynamic Programming
   :srcset: /auto_examples/images/sphx_glr_Practical_4_Dynamic_Programming_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    action space: Discrete(4)
    reward range: (0, 1)
    observation space: Discrete(16)




.. GENERATED FROM PYTHON SOURCE LINES 36-43

.. code-block:: Python



    def uniform_policy(env):
        return np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n
    rld.plot_frozenlake(env=env, policy=uniform_policy(env))





.. image-sg:: /auto_examples/images/sphx_glr_Practical_4_Dynamic_Programming_002.png
   :alt: Practical 4 Dynamic Programming
   :srcset: /auto_examples/images/sphx_glr_Practical_4_Dynamic_Programming_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 44-58

.. code-block:: Python



    def policy_eval_step(env, policy, gamma, v_init=None):
        if v_init is None:
            v_init = np.zeros(env.observation_space.n)
        v = np.zeros(env.observation_space.n)
        for s_from in range(env.observation_space.n):
            for a in range(env.action_space.n):
                pi = policy[s_from, a]
                for p, s_to, r, done in env.P[s_from][a]:
                    v[s_from] += pi * p * (r + gamma * v_init[s_to])
        return v









.. GENERATED FROM PYTHON SOURCE LINES 59-64

.. code-block:: Python



    v = np.zeros(env.observation_space.n)









.. GENERATED FROM PYTHON SOURCE LINES 65-71

.. code-block:: Python



    v = policy_eval_step(env, uniform_policy(env), 1, v)
    rld.plot_frozenlake(env, v, uniform_policy(env), draw_vals=True)





.. image-sg:: /auto_examples/images/sphx_glr_Practical_4_Dynamic_Programming_003.png
   :alt: Practical 4 Dynamic Programming
   :srcset: /auto_examples/images/sphx_glr_Practical_4_Dynamic_Programming_003.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 72-88

.. code-block:: Python



    def policy_eval_step_inplace(env, policy, gamma, v_init=None):
        if v_init is None:
            v_init = np.zeros(env.observation_space.n)
        v = v_init.copy() # opearate on copy in-place
        for s_from in reversed(range(env.observation_space.n)):  # reverse order of states
            v_s_from = 0  # compute value for this state
            for a in range(env.action_space.n):
                pi = policy[s_from, a]
                for p, s_to, r, done in env.P[s_from][a]:
                    v_s_from += pi * p * (r + gamma * v[s_to])  # use the values we also update
            v[s_from] = v_s_from  # update
        return v









.. GENERATED FROM PYTHON SOURCE LINES 89-106

.. code-block:: Python



    def policy_evaluation(env, policy, gamma, v_init=None,
                          print_iter=False, atol=1e-8, max_iter=10**10):
        if v_init is None:
            v_init = np.zeros(env.observation_space.n)
        v = v_init
        for i in range(1, max_iter + 1):
            new_v = policy_eval_step(env, policy, gamma, v)
            if np.allclose(v, new_v, atol=atol):
                break
            v = new_v
        if print_iter:
            print(f"{i} iterations")
        return v









.. GENERATED FROM PYTHON SOURCE LINES 107-117

.. code-block:: Python



    def q_from_v(env, v, s, gamma):
        q = np.zeros(env.action_space.n)
        for a in range(env.action_space.n):
            for p, s_to, r, done in env.P[s][a]:
                q[a] += p * (r + gamma * v[s_to])
        return q









.. GENERATED FROM PYTHON SOURCE LINES 118-134

.. code-block:: Python



    def policy_improvement(env, v, gamma, deterministic=False):
        policy = np.zeros([env.observation_space.n, env.action_space.n]) / env.action_space.n
        for s in range(env.observation_space.n):
            q = q_from_v(env, v, s, gamma)
            if deterministic:
                # deterministic policy
                policy[s][np.argmax(q)] = 1
            else:
                # stochastic policy with equal probability on maximizing actions
                best_a = np.argwhere(q==np.max(q)).flatten()
                policy[s, best_a] = 1 / len(best_a)
        return policy









.. GENERATED FROM PYTHON SOURCE LINES 135-144

.. code-block:: Python



    env = rld.make('FrozenLake8x8-v1', is_slippery=False)
    rld.seed_everything(42, env)
    policy = uniform_policy(env)
    v = policy_evaluation(env, policy, gamma=1)
    rld.plot_frozenlake(env, v, policy, draw_vals=True)





.. image-sg:: /auto_examples/images/sphx_glr_Practical_4_Dynamic_Programming_004.png
   :alt: Practical 4 Dynamic Programming
   :srcset: /auto_examples/images/sphx_glr_Practical_4_Dynamic_Programming_004.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 145-151

.. code-block:: Python



    new_policy = policy_improvement(env, v, gamma=1)
    rld.plot_frozenlake(env, v, new_policy, draw_vals=True)





.. image-sg:: /auto_examples/images/sphx_glr_Practical_4_Dynamic_Programming_005.png
   :alt: Practical 4 Dynamic Programming
   :srcset: /auto_examples/images/sphx_glr_Practical_4_Dynamic_Programming_005.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 152-167

.. code-block:: Python



    gamma = 1
    v = policy_evaluation(env, new_policy, gamma=gamma)
    rld.plot_frozenlake(env, v=v, policy=new_policy, draw_vals=True)
    print(v)
    new_policy = policy_improvement(env, v, gamma=gamma)
    rld.plot_frozenlake(env, v=v, policy=new_policy, draw_vals=True)


    # In[ ]:







.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/images/sphx_glr_Practical_4_Dynamic_Programming_006.png
         :alt: Practical 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Practical_4_Dynamic_Programming_006.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Practical_4_Dynamic_Programming_007.png
         :alt: Practical 4 Dynamic Programming
         :srcset: /auto_examples/images/sphx_glr_Practical_4_Dynamic_Programming_007.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.
     1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1.
     1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0.]





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 1.622 seconds)


.. _sphx_glr_download_auto_examples_Practical_4_Dynamic_Programming.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: Practical_4_Dynamic_Programming.ipynb <Practical_4_Dynamic_Programming.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: Practical_4_Dynamic_Programming.py <Practical_4_Dynamic_Programming.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: Practical_4_Dynamic_Programming.zip <Practical_4_Dynamic_Programming.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
