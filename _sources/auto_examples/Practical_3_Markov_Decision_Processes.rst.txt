
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/Practical_3_Markov_Decision_Processes.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_Practical_3_Markov_Decision_Processes.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_Practical_3_Markov_Decision_Processes.py:


Practical 3: Markov Decision Processes
======================================

.. GENERATED FROM PYTHON SOURCE LINES 6-9

.. code-block:: Python


    # # Practical 3: Markov Decision Processes








.. GENERATED FROM PYTHON SOURCE LINES 10-20

.. code-block:: Python



    import numpy as np
    import matplotlib.pyplot as plt


    # ## Markov Chain

    # Define a transition matrix








.. GENERATED FROM PYTHON SOURCE LINES 21-36

.. code-block:: Python



    n_states = 10
    idx = np.arange(n_states)
    transition_matrix = np.zeros((n_states, n_states))
    transition_matrix[idx, (idx - 1) % n_states] = 0.1
    transition_matrix[idx, (idx + 1) % n_states] = 0.9
    transition_matrix[0, 0] = 0.8
    transition_matrix[0, 1] = 0.1
    transition_matrix[0, -1] = 0.1
    print(f"normalised: {np.all(transition_matrix.sum(axis=1)==1)}")


    # Sample and plot episodes





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    normalised: True




.. GENERATED FROM PYTHON SOURCE LINES 37-56

.. code-block:: Python



    def sample_episodes(n_episodes, length_episodes):
        episodes = np.zeros((n_episodes, length_episodes), dtype=int)
        for idx_episodes in range(n_episodes):
            for idx_time in range(1, length_episodes):
                current_state = episodes[idx_episodes, idx_time - 1]
                probs = transition_matrix[current_state]
                next_state = np.random.choice(n_states, p=probs)
                episodes[idx_episodes, idx_time] = next_state
        return episodes

    episodes = sample_episodes(n_episodes=10, length_episodes=100)
    fig, ax = plt.subplots(1, 1, figsize=(15, 10))
    ax.plot(np.arange(episodes.shape[1]), episodes.T, '-o', alpha=0.3);


    # Plot statistics of state visits




.. image-sg:: /auto_examples/images/sphx_glr_Practical_3_Markov_Decision_Processes_001.png
   :alt: Practical 3 Markov Decision Processes
   :srcset: /auto_examples/images/sphx_glr_Practical_3_Markov_Decision_Processes_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    [<matplotlib.lines.Line2D object at 0x7f315803d940>, <matplotlib.lines.Line2D object at 0x7f315803dca0>, <matplotlib.lines.Line2D object at 0x7f315803dd90>, <matplotlib.lines.Line2D object at 0x7f315803deb0>, <matplotlib.lines.Line2D object at 0x7f315803e120>, <matplotlib.lines.Line2D object at 0x7f315803df70>, <matplotlib.lines.Line2D object at 0x7f315803e330>, <matplotlib.lines.Line2D object at 0x7f315803db80>, <matplotlib.lines.Line2D object at 0x7f315803e270>, <matplotlib.lines.Line2D object at 0x7f315803e4e0>]



.. GENERATED FROM PYTHON SOURCE LINES 57-68

.. code-block:: Python



    episodes = sample_episodes(n_episodes=100, length_episodes=100)
    plt.hist((episodes[:, :50].flatten(), episodes[:, 50:].flatten()),
             density=True,
             label=["first 50 steps", "second 50 steps"])
    plt.legend()


    # Compute the closed-form solution for stationary distribution




.. image-sg:: /auto_examples/images/sphx_glr_Practical_3_Markov_Decision_Processes_002.png
   :alt: Practical 3 Markov Decision Processes
   :srcset: /auto_examples/images/sphx_glr_Practical_3_Markov_Decision_Processes_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7f3158271010>



.. GENERATED FROM PYTHON SOURCE LINES 69-85

.. code-block:: Python



    eigvals, eigvecs = np.linalg.eig(transition_matrix.T)
    stationary = None
    for idx in range(n_states):
        if np.isclose(eigvals[idx], 1):
            stationary = eigvecs[:, idx].real
            stationary /= stationary.sum()
    print(f"Is stationary: {np.all(np.isclose(stationary @ transition_matrix, stationary))}")
    print(stationary)


    # ## Markov Reward Process

    # Add a reward function and estimate the expected reward over time





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Is stationary: True
    [0.47368421 0.05263158 0.05263159 0.05263167 0.05263237 0.05263871
     0.05269575 0.05320915 0.05782976 0.0994152 ]




.. GENERATED FROM PYTHON SOURCE LINES 86-109

.. code-block:: Python



    def reward_function(s):
        return int(s == 0)

    def sample_episodes(n_episodes, length_episodes):
        episodes = np.zeros((n_episodes, length_episodes, 2), dtype=int)
        for idx_episodes in range(n_episodes):
            for idx_time in range(1, length_episodes):
                current_state = episodes[idx_episodes, idx_time - 1, 0]
                probs = transition_matrix[current_state]
                next_state = np.random.choice(n_states, p=probs)
                episodes[idx_episodes, idx_time, 0] = next_state
                episodes[idx_episodes, idx_time, 1] = reward_function(next_state)
        return episodes

    episodes = sample_episodes(n_episodes=1000, length_episodes=100)
    fig, ax = plt.subplots(1, 1, figsize=(15, 10))
    ax.plot(np.arange(episodes.shape[1]), episodes[:, :, 1].mean(axis=0), '-o', alpha=0.3);


    # Add a discount factor and estimate state values




.. image-sg:: /auto_examples/images/sphx_glr_Practical_3_Markov_Decision_Processes_003.png
   :alt: Practical 3 Markov Decision Processes
   :srcset: /auto_examples/images/sphx_glr_Practical_3_Markov_Decision_Processes_003.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    [<matplotlib.lines.Line2D object at 0x7f3157e7f320>]



.. GENERATED FROM PYTHON SOURCE LINES 110-143

.. code-block:: Python



    def sample_episodes(n_episodes, length_episodes, start_state):
        episodes = np.zeros((n_episodes, length_episodes, 2), dtype=int)
        episodes[:, 0, 0] = start_state
        for idx_episodes in range(n_episodes):
            for idx_time in range(1, length_episodes):
                current_state = episodes[idx_episodes, idx_time - 1, 0]
                probs = transition_matrix[current_state]
                next_state = np.random.choice(n_states, p=probs)
                episodes[idx_episodes, idx_time, 0] = next_state
                episodes[idx_episodes, idx_time, 1] = reward_function(next_state)
        return episodes

    def estimate_state_values(*, discount, start_state_list, **kwargs):
        state_values = {}
        for start_state in start_state_list:
            episodes = sample_episodes(start_state=start_state, **kwargs)
            discounted_rewards = episodes[:, :, 1] * np.power(discount, np.arange(episodes.shape[1]))
            returns = discounted_rewards.sum(axis=1)
            value = returns.mean()
            state_values[start_state] = value
            print(f"Value for state {start_state}: {value}")
        return state_values

    state_values = estimate_state_values(discount=0.5,
                                         start_state_list=[n_states - 1, 0, 1],
                                         n_episodes=100, 
                                         length_episodes=10)


    # Compute the closed-form solution for the state values





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Value for state 9: 0.82705078125
    Value for state 0: 0.72212890625
    Value for state 1: 0.0823828125




.. GENERATED FROM PYTHON SOURCE LINES 144-155

.. code-block:: Python



    R = np.zeros(n_states)
    R[0] = 1
    np.linalg.solve(np.eye(n_states) - 0.5 * transition_matrix, 0.5 * transition_matrix @ R)


    # ## MDP

    # Add action to move up/down and update the transition matrix accordingly





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    array([0.74105284, 0.09069124, 0.00808578, 0.00789158, 0.01663844,
           0.03609746, 0.07836786, 0.17013997, 0.36938128, 0.80194284])



.. GENERATED FROM PYTHON SOURCE LINES 156-170

.. code-block:: Python



    n_actions = 2  # up/down
    idx = np.arange(n_states)
    transition_matrix = np.zeros((n_states, n_states, n_actions))
    transition_matrix[idx, (idx - 1) % n_states, 0] = 0.1
    transition_matrix[idx, (idx + 1) % n_states, 0] = 0.9
    transition_matrix[idx, (idx - 1) % n_states, 1] = 0.9
    transition_matrix[idx, (idx + 1) % n_states, 1] = 0.1
    print(f"normalised: {np.all(transition_matrix.sum(axis=1)==1)}")


    # Adapt your sampling routine





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    normalised: True




.. GENERATED FROM PYTHON SOURCE LINES 171-190

.. code-block:: Python



    def sample_episodes(n_episodes, length_episodes, start_state, policy):
        episodes = np.zeros((n_episodes, length_episodes, 3), dtype=int)
        episodes[:, 0, 0] = start_state
        for idx_episodes in range(n_episodes):
            for idx_time in range(1, length_episodes):
                current_state = episodes[idx_episodes, idx_time - 1, 0]
                action = np.random.choice(n_actions, p=policy[current_state])
                probs = transition_matrix[current_state, :, action]
                next_state = np.random.choice(n_states, p=probs)
                episodes[idx_episodes, idx_time, 0] = next_state
                episodes[idx_episodes, idx_time, 1] = reward_function(next_state)
                episodes[idx_episodes, idx_time, 2] = action
        return episodes


    # Define a uniform policy and estimate state values








.. GENERATED FROM PYTHON SOURCE LINES 191-206

.. code-block:: Python



    policy = np.zeros((n_states, n_actions))
    policy[:, 0] = 0.5
    policy[:, 1] = 0.5

    estimate_state_values(discount=0.5,
                          start_state_list=[n_states - 1, 0, 1],
                          n_episodes=100, 
                          length_episodes=10,
                          policy=policy);


    # Change the policy to always go one step up and re-estimate the state values





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Value for state 9: 0.30796875
    Value for state 0: 0.153515625
    Value for state 1: 0.3164453125

    {9: np.float64(0.30796875), 0: np.float64(0.153515625), 1: np.float64(0.3164453125)}



.. GENERATED FROM PYTHON SOURCE LINES 207-222

.. code-block:: Python



    policy = np.zeros((n_states, n_actions))
    policy[:, 0] = 1
    policy[:, 1] = 0

    estimate_state_values(discount=0.5,
                          start_state_list=[n_states - 1, 0, 1],
                          n_episodes=100, 
                          length_episodes=10,
                          policy=policy);


    # Experiment with different policies and try to improve them





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Value for state 9: 0.478203125
    Value for state 0: 0.0597265625
    Value for state 1: 0.0657421875

    {9: np.float64(0.478203125), 0: np.float64(0.0597265625), 1: np.float64(0.0657421875)}



.. GENERATED FROM PYTHON SOURCE LINES 223-237

.. code-block:: Python



    policy = np.zeros((n_states, n_actions))
    policy[:int(n_states / 2), 0] = 0
    policy[:int(n_states / 2), 1] = 1
    policy[int(n_states / 2):, 0] = 1
    policy[int(n_states / 2):, 1] = 0

    estimate_state_values(discount=0.5,
                          start_state_list=[n_states - 1, 0, 1],
                          n_episodes=100, 
                          length_episodes=10,
                          policy=policy);





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Value for state 9: 0.60015625
    Value for state 0: 0.299765625
    Value for state 1: 0.587109375

    {9: np.float64(0.60015625), 0: np.float64(0.299765625), 1: np.float64(0.587109375)}




.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 2.164 seconds)


.. _sphx_glr_download_auto_examples_Practical_3_Markov_Decision_Processes.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: Practical_3_Markov_Decision_Processes.ipynb <Practical_3_Markov_Decision_Processes.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: Practical_3_Markov_Decision_Processes.py <Practical_3_Markov_Decision_Processes.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: Practical_3_Markov_Decision_Processes.zip <Practical_3_Markov_Decision_Processes.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
