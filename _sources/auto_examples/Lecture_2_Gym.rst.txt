
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/Lecture_2_Gym.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_Lecture_2_Gym.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_Lecture_2_Gym.py:


Lecture 2: Gym
==============

.. GENERATED FROM PYTHON SOURCE LINES 6-9

.. code-block:: Python


    # # Lecture 2: Gym








.. GENERATED FROM PYTHON SOURCE LINES 10-14

Dependencies and setup
----------------------

(this can take a minute or so...)

.. GENERATED FROM PYTHON SOURCE LINES 16-25

.. code-block:: Python



    # !pip install swig

    # !pip install rldurham  # latest release
    # !pip install git+https://github.com/robert-lieck/rldurham.git@main  # latest main version (typically same as release)
    # !pip install git+https://github.com/robert-lieck/rldurham.git@dev  # latest dev version









.. GENERATED FROM PYTHON SOURCE LINES 26-32

.. code-block:: Python



    import gymnasium as gym
    import rldurham as rld  # Reinforcement Learning Durham package with helper functions









.. GENERATED FROM PYTHON SOURCE LINES 33-37

Basic environment
-----------------



.. GENERATED FROM PYTHON SOURCE LINES 39-55

.. code-block:: Python



    env = gym.make('CartPole-v1', render_mode="human")
    observation, info = env.reset(seed=42)

    for episode in range(10):
        observation, info = env.reset()
        done = False
        while not done:
            action = env.action_space.sample()  # random action
            observation, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated

    env.close()









.. GENERATED FROM PYTHON SOURCE LINES 56-60

Reinforcement Learning Durham:
------------------------------

_rldurham_ Python package

.. GENERATED FROM PYTHON SOURCE LINES 62-129

.. code-block:: Python



    # drop-in for gym.make that enables logging (use render_mode="rgb_array" to enable video rendering)
    env = rld.make('CartPole-v1', render_mode="rgb_array")

    # record statistics (returned in info) and videos
    env = rld.Recorder(
        env,
        smoothing=10,                       # track rolling averages (not required for coursework)
        video=True,                         # enable recording videos
        video_folder="videos",              # folder for videos
        video_prefix="xxxx00-agent-video",  # prefix for videos
        logs=True,                          # keep logs
    )

    # make reproducible by seeding everything (python, numpy, pytorch, env)
    # this also calls env.reset
    seed, observation, info = rld.seed_everything(42, env)

    # optionally track statistics for plotting
    tracker = rld.InfoTracker()

    # run episodes
    for episode in range(11):
        # recording statistics and video can be switched on and off (video recording is slow!)
        env.info = episode % 2 == 0   # track every other episode (usually tracking every episode is fine)
        env.video = episode % 4 == 0  # you only want to record videos every x episodes (set BEFORE calling reset!)
        #######################################################################
        # this is the same as above
        observation, info = env.reset()
        done = False
        while not done:
            action = env.action_space.sample()  # Random action
            observation, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
        #######################################################################
            if done:
                # per-episode statistics are returned by Recorder wrapper in info
                #     'idx': index/count of the episode
                #     'length': length of the episode
                #     'r_sum': sum of rewards in episode
                #     'r_mean': mean of rewards in episode
                #     'r_std': standard deviation of rewards in episode
                #     'length_': average `length' over smoothing window
                #     'r_sum_': reward sum over smoothing window (not the average)
                #     'r_mean_': average reward sum per episode over smoothing window (i.e. average of `r_sum')
                #     'r_std_': standard deviation of reward sum per episode over smoothing window

                # InfoTracker turns these into arrays over time
                tracker.track(info)
                print(tracker.info)

                # some plotting functionality is provided (this will refresh in notebooks)
                #  - combinations of ``r_mean`` and ``r_std`` or ``r_mean_`` and ``r_std_`` are most insightful
                #  - for `CartPole`, ``length`` and ``r_sum`` are the same as there is a unit reward for each
                #    time step of successful balancing
                tracker.plot(r_mean_=True, r_std_=True,
                             length=dict(linestyle='--', marker='o'),
                             r_sum=dict(linestyle='', marker='x'))

    # don't forget to close environment (e.g. triggers last video save)
    env.close()

    # write log file (for coursework)
    env.write_log(folder="logs", file="xxxx00-agent-log.txt")





.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_001.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_001.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_002.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_002.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_003.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_003.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_004.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_004.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_005.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_005.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_006.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_006.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_007.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_007.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_008.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_008.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_009.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_009.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_010.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_010.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_011.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_011.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/.local/lib/python3.12/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: WARN: Overwriting existing videos at /home/runner/work/rldurham/rldurham/examples/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)
      logger.warn(
    {'recorder': {'idx': [0], 'length': [29], 'r_sum': [29.0], 'r_mean': [1.0], 'r_std': [0.0], 'length_': [29.0], 'r_sum_': [29.0], 'r_mean_': [29.0], 'r_std_': [0.0]}}
            {'recorder': {'idx': [0], 'length': [29], 'r_sum': [29.0], 'r_mean': [1.0], 'r_std': [0.0], 'length_': [29.0], 'r_sum_': [29.0], 'r_mean_': [29.0], 'r_std_': [0.0]}}
            {'recorder': {'idx': [0, 2], 'length': [29, 69], 'r_sum': [29.0, 69.0], 'r_mean': [1.0, 1.0], 'r_std': [0.0, 0.0], 'length_': [29.0, 49.0], 'r_sum_': [29.0, 98.0], 'r_mean_': [29.0, 49.0], 'r_std_': [0.0, 20.0]}}
            {'recorder': {'idx': [0, 2], 'length': [29, 69], 'r_sum': [29.0, 69.0], 'r_mean': [1.0, 1.0], 'r_std': [0.0, 0.0], 'length_': [29.0, 49.0], 'r_sum_': [29.0, 98.0], 'r_mean_': [29.0, 49.0], 'r_std_': [0.0, 20.0]}}
            {'recorder': {'idx': [0, 2, 4], 'length': [29, 69, 39], 'r_sum': [29.0, 69.0, 39.0], 'r_mean': [1.0, 1.0, 1.0], 'r_std': [0.0, 0.0, 0.0], 'length_': [29.0, 49.0, 45.666666666666664], 'r_sum_': [29.0, 98.0, 137.0], 'r_mean_': [29.0, 49.0, 45.666666666666664], 'r_std_': [0.0, 20.0, 16.996731711975958]}}
            {'recorder': {'idx': [0, 2, 4], 'length': [29, 69, 39], 'r_sum': [29.0, 69.0, 39.0], 'r_mean': [1.0, 1.0, 1.0], 'r_std': [0.0, 0.0, 0.0], 'length_': [29.0, 49.0, 45.666666666666664], 'r_sum_': [29.0, 98.0, 137.0], 'r_mean_': [29.0, 49.0, 45.666666666666664], 'r_std_': [0.0, 20.0, 16.996731711975958]}}
            {'recorder': {'idx': [0, 2, 4, 6], 'length': [29, 69, 39, 14], 'r_sum': [29.0, 69.0, 39.0, 14.0], 'r_mean': [1.0, 1.0, 1.0, 1.0], 'r_std': [0.0, 0.0, 0.0, 0.0], 'length_': [29.0, 49.0, 45.666666666666664, 37.75], 'r_sum_': [29.0, 98.0, 137.0, 151.0], 'r_mean_': [29.0, 49.0, 45.666666666666664, 37.75], 'r_std_': [0.0, 20.0, 16.996731711975958, 20.116846174288852]}}
            {'recorder': {'idx': [0, 2, 4, 6], 'length': [29, 69, 39, 14], 'r_sum': [29.0, 69.0, 39.0, 14.0], 'r_mean': [1.0, 1.0, 1.0, 1.0], 'r_std': [0.0, 0.0, 0.0, 0.0], 'length_': [29.0, 49.0, 45.666666666666664, 37.75], 'r_sum_': [29.0, 98.0, 137.0, 151.0], 'r_mean_': [29.0, 49.0, 45.666666666666664, 37.75], 'r_std_': [0.0, 20.0, 16.996731711975958, 20.116846174288852]}}
            {'recorder': {'idx': [0, 2, 4, 6, 8], 'length': [29, 69, 39, 14, 28], 'r_sum': [29.0, 69.0, 39.0, 14.0, 28.0], 'r_mean': [1.0, 1.0, 1.0, 1.0, 1.0], 'r_std': [0.0, 0.0, 0.0, 0.0, 0.0], 'length_': [29.0, 49.0, 45.666666666666664, 37.75, 35.8], 'r_sum_': [29.0, 98.0, 137.0, 151.0, 179.0], 'r_mean_': [29.0, 49.0, 45.666666666666664, 37.75, 35.8], 'r_std_': [0.0, 20.0, 16.996731711975958, 20.116846174288852, 18.410866356584094]}}
            {'recorder': {'idx': [0, 2, 4, 6, 8], 'length': [29, 69, 39, 14, 28], 'r_sum': [29.0, 69.0, 39.0, 14.0, 28.0], 'r_mean': [1.0, 1.0, 1.0, 1.0, 1.0], 'r_std': [0.0, 0.0, 0.0, 0.0, 0.0], 'length_': [29.0, 49.0, 45.666666666666664, 37.75, 35.8], 'r_sum_': [29.0, 98.0, 137.0, 151.0, 179.0], 'r_mean_': [29.0, 49.0, 45.666666666666664, 37.75, 35.8], 'r_std_': [0.0, 20.0, 16.996731711975958, 20.116846174288852, 18.410866356584094]}}
            {'recorder': {'idx': [0, 2, 4, 6, 8, 10], 'length': [29, 69, 39, 14, 28, 25], 'r_sum': [29.0, 69.0, 39.0, 14.0, 28.0, 25.0], 'r_mean': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'r_std': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'length_': [29.0, 49.0, 45.666666666666664, 37.75, 35.8, 34.0], 'r_sum_': [29.0, 98.0, 137.0, 151.0, 179.0, 204.0], 'r_mean_': [29.0, 49.0, 45.666666666666664, 37.75, 35.8, 34.0], 'r_std_': [0.0, 20.0, 16.996731711975958, 20.116846174288852, 18.410866356584094, 17.281975195754296]}}
        



.. GENERATED FROM PYTHON SOURCE LINES 130-139

.. code-block:: Python



    # print log file
    import pandas as pd
    log_file = "logs/xxxx00-agent-log.txt"
    print(f"log file: {log_file}")
    print(pd.read_csv(log_file, sep="\t").head())






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    log file: logs/xxxx00-agent-log.txt
       count  reward_sum  squared_reward_sum  length
    0      0        29.0                29.0      29
    1      1        17.0                17.0      17
    2      2        69.0                69.0      69
    3      3        15.0                15.0      15
    4      4        39.0                39.0      39




.. GENERATED FROM PYTHON SOURCE LINES 140-150

.. code-block:: Python



    # show video
    import os, re
    from ipywidgets import Video
    video_file = "videos/" + sorted(f for f in os.listdir("videos") if re.match(r".+episode=4.+\.mp4", f))[0]
    print(f"video file: {video_file}")
    Video.from_file(video_file)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    video file: videos/xxxx00-agent-video,episode=4,score=39.0.mp4

    Video(value=b'\x00\x00\x00 ftypisom\x00\x00\x02\x00isomiso2avc1mp41\x00\x00\x00\x08free...')



.. GENERATED FROM PYTHON SOURCE LINES 151-155

Different environments
----------------------



.. GENERATED FROM PYTHON SOURCE LINES 157-196

.. code-block:: Python



    ## render mode
    # rm = "human"     # for rendering in separate window
    rm = 'rgb_array' # for recording videos or rendering in notebook
    # rm = None        # no rendering

    ## select environment
    env = rld.make('CartPole-v1', render_mode=rm)              # easy discrete
    # env = rld.make('FrozenLake-v1', render_mode=rm)            # easy discrete
    # env = rld.make('LunarLander-v3', render_mode=rm)           # discrete
    # env = rld.make('ALE/Breakout-v5', render_mode=rm)          # discrete (atari)
    # env = rld.make('Pong-ram-v4', render_mode=rm)              # discrete (atari)
    # env = rld.make('Gravitar-ram-v4', render_mode=rm)          # hard discrete (atari)
    #
    # env = rld.make('Pendulum-v1', render_mode=rm)              # easy continuous
    # env = rld.make('LunarLanderContinuous-v3', render_mode=rm) # continuous
    # env = rld.make('BipedalWalker-v3', render_mode=rm)         # continuous
    # env = rld.make('BipedalWalkerHardcore-v3', render_mode=rm) # hard continuous

    ## wrap for stats and video recording (requires render_mode='rgb_array')
    # env = rld.Recorder(env, video=True)
    # env.video = False  # deactivate

    ## there are many more!
    # gym.pprint_registry()

    # get some info
    discrete_act = hasattr(env.action_space, 'n')
    discrete_obs = hasattr(env.observation_space, 'n')
    act_dim = env.action_space.n if discrete_act else env.action_space.shape[0]
    obs_dim = env.observation_space.n if discrete_obs else env.observation_space.shape[0]
    discrete_act, discrete_obs, act_dim, obs_dim = rld.env_info(env)  # same
    print(f"The environment has "
          f"{act_dim}{(f' discrete' if discrete_act else f'-dimensional continuous')} actions and "
          f"{obs_dim}{(f' discrete' if discrete_obs else f'-dimensional continuous')} observations.")
    print('The maximum timesteps is: {}'.format(env.spec.max_episode_steps))






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The environment has 2 discrete actions and 4-dimensional continuous observations.
    The maximum timesteps is: 500




.. GENERATED FROM PYTHON SOURCE LINES 197-213

.. code-block:: Python



    for episode in range(1):
        observation, info = env.reset()
        rld.render(env, clear=True)
        done = False
        for step in range(200):
            action = env.action_space.sample()  # random action
            observation, reward, terminated, truncated, info = env.step(action)
            rld.render(env, clear=True)
            done = terminated or truncated
            if done:
                break
    env.close()





.. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_012.png
   :alt: Lecture 2 Gym
   :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_012.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                                                                                                                                        



.. GENERATED FROM PYTHON SOURCE LINES 214-218

Training an agent
-----------------



.. GENERATED FROM PYTHON SOURCE LINES 220-230

.. code-block:: Python



    import numpy as np
    import torch

    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    print('The device is: {}'.format(device))
    if device.type != 'cpu': print('It\'s recommended to train on the cpu for this')






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The device is: cpu




.. GENERATED FROM PYTHON SOURCE LINES 231-254

.. code-block:: Python



    class Agent(torch.nn.Module):
        def __init__(self, env):
            super().__init__()
            self.discrete_act, self.discrete_obs, self.act_dim, self.obs_dim = rld.env_info(env)

        def prob_action(self, obs):
            return np.ones(self.act_dim)/self.act_dim

        def sample_action(self, prob):
            if self.discrete_act:
                return np.random.choice(self.act_dim, p=prob)
            else:
                return np.random.uniform(-1.0, 1.0, size=self.act_dim)

        def train(self):
            return

        def put_data(self, item):
            return









.. GENERATED FROM PYTHON SOURCE LINES 255-295

.. code-block:: Python



    # init environment
    env = rld.make('CartPole-v1')
    env = rld.Recorder(env, smoothing=10)

    # seed
    rld.seed_everything(seed=42, env=env)

    # init agent
    agent = Agent(env)

    # training procedure
    tracker = rld.InfoTracker()
    for episode in range(201):
        obs, info = env.reset()
        done = False

        # get episode
        while not done:
            # select action
            prob = agent.prob_action(obs)
            action = agent.sample_action(prob)

            # take action in environment
            next_obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            agent.put_data((reward, prob[action] if agent.discrete_act else None))
            obs = next_obs

        # track and plot
        tracker.track(info)
        if episode % 10 == 0:
            tracker.plot(r_mean_=True, r_std_=True)

        # update agent's policy
        agent.train()
    env.close()





.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_013.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_013.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_014.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_014.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_015.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_015.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_016.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_016.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_017.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_017.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_018.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_018.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_019.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_019.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_020.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_020.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_021.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_021.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_022.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_022.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_023.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_023.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_024.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_024.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_025.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_025.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_026.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_026.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_027.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_027.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_028.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_028.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_029.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_029.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_030.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_030.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_031.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_031.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_032.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_032.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_Lecture_2_Gym_033.png
         :alt: Lecture 2 Gym
         :srcset: /auto_examples/images/sphx_glr_Lecture_2_Gym_033.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                                                                                                                                                                    /home/runner/work/rldurham/rldurham/rldurham/__init__.py:474: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
      fig, ax = plt.subplots(1, 1)
        



.. GENERATED FROM PYTHON SOURCE LINES 296-303

REINFORCE agent example
-----------------------


This code is based on: https://github.com/seungeunrho/minimalRL

Note: these implementations are good to study, although most are for discrete action spaces

.. GENERATED FROM PYTHON SOURCE LINES 305-348

.. code-block:: Python



    # this is an implementation of REINFORCE (taught in lecture 8) - one of the simplest classical policy gradient methods
    # this will only work for simple discrete control problems like cart pole or (slowly) lunar lander discrete
    plot_interval = 50
    video_every = 500
    max_episodes = 5000


    class Agent(torch.nn.Module):
        def __init__(self, env):
            super().__init__()
            discrete_act, discrete_obs, act_dim, obs_dim = rld.env_info(env)
            if not discrete_act:
                raise RuntimeError("REINFORCE only works for discrete action spaces")
            self.data = []
            self.fc1 = torch.nn.Linear(obs_dim, 128)
            self.fc2 = torch.nn.Linear(128, act_dim)
            self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0002)

        def sample_action(self, prob):
            m = torch.Categorical(prob)
            a = m.sample()
            return a.item()

        def prob_action(self, s):
            x = torch.F.relu(self.fc1(torch.from_numpy(s).float()))
            return torch.F.softmax(self.fc2(x), dim=0)

        def put_data(self, item):
            self.data.append(item)

        def train(self):
            R = 0
            self.optimizer.zero_grad()
            for r, prob in self.data[::-1]:
                R = r + 0.98 * R
                loss = -torch.log(prob) * R
                loss.backward()
            self.optimizer.step()
            self.data = []









.. GENERATED FROM PYTHON SOURCE LINES 349-353

Custom environments (here: multi-armed bandits)
-----------------------------------------------



.. GENERATED FROM PYTHON SOURCE LINES 355-397

.. code-block:: Python



    # one armed bandits are slot machines where you can win or loose money
    # some casinos tune them, so some machines are less successful (the probability distribution of winning)
    # the machines also dish out varying rewards (the reward distribution)
    # so basically each machine has some probability of dishing out £ reward (p_dist doesn't need to sum to 1) else it gives £0

    class BanditEnv(gym.Env):

        def __init__(self, p_dist=[0.4,0.2,0.1,0.1,0.1,0.7], r_dist=[1,0.1,2,0.5,6,70]):

            self.p_dist = p_dist
            self.r_dist = r_dist

            self.n_bandits = len(p_dist)
            self.action_space = gym.spaces.Discrete(self.n_bandits)
            self.observation_space = gym.spaces.Discrete(1)

        def step(self, action):
            assert self.action_space.contains(action)

            reward = -25
            terminated = True
            truncated = False

            if np.random.uniform() < self.p_dist[action]:
                if not isinstance(self.r_dist[action], list):
                    reward += self.r_dist[action]
                else:
                    reward += np.random.normal(self.r_dist[action][0], self.r_dist[action][1])

            return np.zeros(1), reward, terminated, truncated, {}

        def reset(self, seed=None, options=None):
            if seed is not None:
                self.np_random, seed = gym.utils.seeding.np_random(seed)
            return np.zeros(1), {}

        def render(self):
            pass









.. GENERATED FROM PYTHON SOURCE LINES 398-420

.. code-block:: Python



    # initialise
    discrete = True
    env = BanditEnv()
    obs_dim = 1
    act_dim = len(env.p_dist)
    env.spec = gym.envs.registration.EnvSpec('BanditEnv-v0', max_episode_steps=5)
    max_episodes = 1000

    for episode in range(10):
        observation, info = env.reset()
        done = False
        while not done:
            action = env.action_space.sample()  # random action
            observation, reward, terminated, truncated, info = env.step(action)
            print(action, reward)
            done = terminated or truncated

    env.close()






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    0 -25
    3 -25
    2 -23
    0 -25
    2 -25
    2 -25
    3 -25
    1 -24.9
    3 -25
    4 -25




.. GENERATED FROM PYTHON SOURCE LINES 421-426

.. code-block:: Python



    gym.register(id="BanditEnv-v0", entry_point=BanditEnv)
    gym.pprint_registry()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ===== classic_control =====
    Acrobot-v1                  CartPole-v0                 CartPole-v1
    MountainCar-v0              MountainCarContinuous-v0    Pendulum-v1
    ===== phys2d =====
    phys2d/CartPole-v0          phys2d/CartPole-v1          phys2d/Pendulum-v0
    ===== box2d =====
    BipedalWalker-v3            BipedalWalkerHardcore-v3    CarRacing-v3
    LunarLander-v3              LunarLanderContinuous-v3
    ===== toy_text =====
    Blackjack-v1                CliffWalking-v1             CliffWalkingSlippery-v1
    FrozenLake-v1               FrozenLake8x8-v1            Taxi-v3
    ===== tabular =====
    tabular/Blackjack-v0        tabular/CliffWalking-v0
    ===== None =====
    Ant-v2                      Ant-v3                      BanditEnv-v0
    GymV21Environment-v0        GymV26Environment-v0        HalfCheetah-v2
    HalfCheetah-v3              Hopper-v2                   Hopper-v3
    Humanoid-v2                 Humanoid-v3                 HumanoidStandup-v2
    InvertedDoublePendulum-v2   InvertedPendulum-v2         Pusher-v2
    Reacher-v2                  Swimmer-v2                  Swimmer-v3
    Walker2d-v2                 Walker2d-v3
    ===== mujoco =====
    Ant-v4                      Ant-v5                      HalfCheetah-v4
    HalfCheetah-v5              Hopper-v4                   Hopper-v5
    Humanoid-v4                 Humanoid-v5                 HumanoidStandup-v4
    HumanoidStandup-v5          InvertedDoublePendulum-v4   InvertedDoublePendulum-v5
    InvertedPendulum-v4         InvertedPendulum-v5         Pusher-v4
    Pusher-v5                   Reacher-v4                  Reacher-v5
    Swimmer-v4                  Swimmer-v5                  Walker2d-v4
    Walker2d-v5
    ===== env =====
    Adventure-v0                Adventure-v4                AdventureNoFrameskip-v0
    AdventureNoFrameskip-v4     AirRaid-v0                  AirRaid-v4
    AirRaidNoFrameskip-v0       AirRaidNoFrameskip-v4       Alien-v0
    Alien-v4                    AlienNoFrameskip-v0         AlienNoFrameskip-v4
    Amidar-v0                   Amidar-v4                   AmidarNoFrameskip-v0
    AmidarNoFrameskip-v4        Assault-v0                  Assault-v4
    AssaultNoFrameskip-v0       AssaultNoFrameskip-v4       Asterix-v0
    Asterix-v4                  AsterixNoFrameskip-v0       AsterixNoFrameskip-v4
    Asteroids-v0                Asteroids-v4                AsteroidsNoFrameskip-v0
    AsteroidsNoFrameskip-v4     Atlantis-v0                 Atlantis-v4
    AtlantisNoFrameskip-v0      AtlantisNoFrameskip-v4      BankHeist-v0
    BankHeist-v4                BankHeistNoFrameskip-v0     BankHeistNoFrameskip-v4
    BattleZone-v0               BattleZone-v4               BattleZoneNoFrameskip-v0
    BattleZoneNoFrameskip-v4    BeamRider-v0                BeamRider-v4
    BeamRiderNoFrameskip-v0     BeamRiderNoFrameskip-v4     Berzerk-v0
    Berzerk-v4                  BerzerkNoFrameskip-v0       BerzerkNoFrameskip-v4
    Bowling-v0                  Bowling-v4                  BowlingNoFrameskip-v0
    BowlingNoFrameskip-v4       Boxing-v0                   Boxing-v4
    BoxingNoFrameskip-v0        BoxingNoFrameskip-v4        Breakout-v0
    Breakout-v4                 BreakoutNoFrameskip-v0      BreakoutNoFrameskip-v4
    Carnival-v0                 Carnival-v4                 CarnivalNoFrameskip-v0
    CarnivalNoFrameskip-v4      Centipede-v0                Centipede-v4
    CentipedeNoFrameskip-v0     CentipedeNoFrameskip-v4     ChopperCommand-v0
    ChopperCommand-v4           ChopperCommandNoFrameskip-v0 ChopperCommandNoFrameskip-v4
    CrazyClimber-v0             CrazyClimber-v4             CrazyClimberNoFrameskip-v0
    CrazyClimberNoFrameskip-v4  Defender-v0                 Defender-v4
    DefenderNoFrameskip-v0      DefenderNoFrameskip-v4      DemonAttack-v0
    DemonAttack-v4              DemonAttackNoFrameskip-v0   DemonAttackNoFrameskip-v4
    DoubleDunk-v0               DoubleDunk-v4               DoubleDunkNoFrameskip-v0
    DoubleDunkNoFrameskip-v4    ElevatorAction-v0           ElevatorAction-v4
    ElevatorActionNoFrameskip-v0 ElevatorActionNoFrameskip-v4 Enduro-v0
    Enduro-v4                   EnduroNoFrameskip-v0        EnduroNoFrameskip-v4
    FishingDerby-v0             FishingDerby-v4             FishingDerbyNoFrameskip-v0
    FishingDerbyNoFrameskip-v4  Freeway-v0                  Freeway-v4
    FreewayNoFrameskip-v0       FreewayNoFrameskip-v4       Frostbite-v0
    Frostbite-v4                FrostbiteNoFrameskip-v0     FrostbiteNoFrameskip-v4
    Gopher-v0                   Gopher-v4                   GopherNoFrameskip-v0
    GopherNoFrameskip-v4        Gravitar-v0                 Gravitar-v4
    GravitarNoFrameskip-v0      GravitarNoFrameskip-v4      Hero-v0
    Hero-v4                     HeroNoFrameskip-v0          HeroNoFrameskip-v4
    IceHockey-v0                IceHockey-v4                IceHockeyNoFrameskip-v0
    IceHockeyNoFrameskip-v4     Jamesbond-v0                Jamesbond-v4
    JamesbondNoFrameskip-v0     JamesbondNoFrameskip-v4     JourneyEscape-v0
    JourneyEscape-v4            JourneyEscapeNoFrameskip-v0 JourneyEscapeNoFrameskip-v4
    Kangaroo-v0                 Kangaroo-v4                 KangarooNoFrameskip-v0
    KangarooNoFrameskip-v4      Krull-v0                    Krull-v4
    KrullNoFrameskip-v0         KrullNoFrameskip-v4         KungFuMaster-v0
    KungFuMaster-v4             KungFuMasterNoFrameskip-v0  KungFuMasterNoFrameskip-v4
    MontezumaRevenge-v0         MontezumaRevenge-v4         MontezumaRevengeNoFrameskip-v0
    MontezumaRevengeNoFrameskip-v4 MsPacman-v0                 MsPacman-v4
    MsPacmanNoFrameskip-v0      MsPacmanNoFrameskip-v4      NameThisGame-v0
    NameThisGame-v4             NameThisGameNoFrameskip-v0  NameThisGameNoFrameskip-v4
    Phoenix-v0                  Phoenix-v4                  PhoenixNoFrameskip-v0
    PhoenixNoFrameskip-v4       Pitfall-v0                  Pitfall-v4
    PitfallNoFrameskip-v0       PitfallNoFrameskip-v4       Pong-v0
    Pong-v4                     PongNoFrameskip-v0          PongNoFrameskip-v4
    Pooyan-v0                   Pooyan-v4                   PooyanNoFrameskip-v0
    PooyanNoFrameskip-v4        PrivateEye-v0               PrivateEye-v4
    PrivateEyeNoFrameskip-v0    PrivateEyeNoFrameskip-v4    Qbert-v0
    Qbert-v4                    QbertNoFrameskip-v0         QbertNoFrameskip-v4
    Riverraid-v0                Riverraid-v4                RiverraidNoFrameskip-v0
    RiverraidNoFrameskip-v4     RoadRunner-v0               RoadRunner-v4
    RoadRunnerNoFrameskip-v0    RoadRunnerNoFrameskip-v4    Robotank-v0
    Robotank-v4                 RobotankNoFrameskip-v0      RobotankNoFrameskip-v4
    Seaquest-v0                 Seaquest-v4                 SeaquestNoFrameskip-v0
    SeaquestNoFrameskip-v4      Skiing-v0                   Skiing-v4
    SkiingNoFrameskip-v0        SkiingNoFrameskip-v4        Solaris-v0
    Solaris-v4                  SolarisNoFrameskip-v0       SolarisNoFrameskip-v4
    SpaceInvaders-v0            SpaceInvaders-v4            SpaceInvadersNoFrameskip-v0
    SpaceInvadersNoFrameskip-v4 StarGunner-v0               StarGunner-v4
    StarGunnerNoFrameskip-v0    StarGunnerNoFrameskip-v4    Tennis-v0
    Tennis-v4                   TennisNoFrameskip-v0        TennisNoFrameskip-v4
    TimePilot-v0                TimePilot-v4                TimePilotNoFrameskip-v0
    TimePilotNoFrameskip-v4     Tutankham-v0                Tutankham-v4
    TutankhamNoFrameskip-v0     TutankhamNoFrameskip-v4     UpNDown-v0
    UpNDown-v4                  UpNDownNoFrameskip-v0       UpNDownNoFrameskip-v4
    Venture-v0                  Venture-v4                  VentureNoFrameskip-v0
    VentureNoFrameskip-v4       VideoPinball-v0             VideoPinball-v4
    VideoPinballNoFrameskip-v0  VideoPinballNoFrameskip-v4  WizardOfWor-v0
    WizardOfWor-v4              WizardOfWorNoFrameskip-v0   WizardOfWorNoFrameskip-v4
    YarsRevenge-v0              YarsRevenge-v4              YarsRevengeNoFrameskip-v0
    YarsRevengeNoFrameskip-v4   Zaxxon-v0                   Zaxxon-v4
    ZaxxonNoFrameskip-v0        ZaxxonNoFrameskip-v4
    ===== ALE =====
    ALE/Adventure-v5            ALE/AirRaid-v5              ALE/Alien-v5
    ALE/Amidar-v5               ALE/Assault-v5              ALE/Asterix-v5
    ALE/Asteroids-v5            ALE/Atlantis-v5             ALE/Atlantis2-v5
    ALE/Backgammon-v5           ALE/BankHeist-v5            ALE/BasicMath-v5
    ALE/BattleZone-v5           ALE/BeamRider-v5            ALE/Berzerk-v5
    ALE/Blackjack-v5            ALE/Bowling-v5              ALE/Boxing-v5
    ALE/Breakout-v5             ALE/Carnival-v5             ALE/Casino-v5
    ALE/Centipede-v5            ALE/ChopperCommand-v5       ALE/CrazyClimber-v5
    ALE/Crossbow-v5             ALE/Darkchambers-v5         ALE/Defender-v5
    ALE/DemonAttack-v5          ALE/DonkeyKong-v5           ALE/DoubleDunk-v5
    ALE/Earthworld-v5           ALE/ElevatorAction-v5       ALE/Enduro-v5
    ALE/Entombed-v5             ALE/Et-v5                   ALE/FishingDerby-v5
    ALE/FlagCapture-v5          ALE/Freeway-v5              ALE/Frogger-v5
    ALE/Frostbite-v5            ALE/Galaxian-v5             ALE/Gopher-v5
    ALE/Gravitar-v5             ALE/Hangman-v5              ALE/HauntedHouse-v5
    ALE/Hero-v5                 ALE/HumanCannonball-v5      ALE/IceHockey-v5
    ALE/Jamesbond-v5            ALE/JourneyEscape-v5        ALE/Kaboom-v5
    ALE/Kangaroo-v5             ALE/KeystoneKapers-v5       ALE/KingKong-v5
    ALE/Klax-v5                 ALE/Koolaid-v5              ALE/Krull-v5
    ALE/KungFuMaster-v5         ALE/LaserGates-v5           ALE/LostLuggage-v5
    ALE/MarioBros-v5            ALE/MiniatureGolf-v5        ALE/MontezumaRevenge-v5
    ALE/MrDo-v5                 ALE/MsPacman-v5             ALE/NameThisGame-v5
    ALE/Othello-v5              ALE/Pacman-v5               ALE/Phoenix-v5
    ALE/Pitfall-v5              ALE/Pitfall2-v5             ALE/Pong-v5
    ALE/Pooyan-v5               ALE/PrivateEye-v5           ALE/Qbert-v5
    ALE/Riverraid-v5            ALE/RoadRunner-v5           ALE/Robotank-v5
    ALE/Seaquest-v5             ALE/SirLancelot-v5          ALE/Skiing-v5
    ALE/Solaris-v5              ALE/SpaceInvaders-v5        ALE/SpaceWar-v5
    ALE/StarGunner-v5           ALE/Superman-v5             ALE/Surround-v5
    ALE/Tennis-v5               ALE/Tetris-v5               ALE/TicTacToe3D-v5
    ALE/TimePilot-v5            ALE/Trondead-v5             ALE/Turmoil-v5
    ALE/Tutankham-v5            ALE/UpNDown-v5              ALE/Venture-v5
    ALE/VideoCheckers-v5        ALE/VideoChess-v5           ALE/VideoCube-v5
    ALE/VideoPinball-v5         ALE/WizardOfWor-v5          ALE/WordZapper-v5
    ALE/YarsRevenge-v5          ALE/Zaxxon-v5
    ===== rldurham =====
    rldurham/Walker





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 7.375 seconds)


.. _sphx_glr_download_auto_examples_Lecture_2_Gym.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: Lecture_2_Gym.ipynb <Lecture_2_Gym.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: Lecture_2_Gym.py <Lecture_2_Gym.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: Lecture_2_Gym.zip <Lecture_2_Gym.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
