{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# imports\nimport gymnasium as gym\nimport time\nimport itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\nfrom collections import defaultdict\nimport rldurham as rld"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# actions\nLEFT, DOWN, RIGHT, UP = 0,1,2,3\n\n# import the frozen lake gym environment\nname = 'FrozenLake-v1'\nenv = rld.make(name, is_slippery=False) # warning: setting slippery=True results in very complex environment dynamics where the optimal solution does not make sense to humans!\nrld.seed_everything(42, env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TD(0) prediction\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_episodes = 10000\nalpha = 0.2\ngamma = 1.0\nV = np.zeros([env.observation_space.n])\n\nfor episode in range(1,num_episodes+1):\n    s, _ = env.reset()\n    while (True):\n        a = np.ones(env.action_space.n, dtype=float) / env.action_space.n # random policy\n        a = np.random.choice(len(a), p=a)\n        next_s, reward, term, trun, _ = env.step(a)\n        done = term or trun\n        V[s] += alpha * (reward + gamma * V[next_s] - V[s])\n        if done: break\n        s = next_s\n\n\n# ...this is just prediction, it doesn't solve the control problem - let's plot the policy learned from all this random walking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# helper code to plot the policy greedily obtained from TD(0) - ignore this!\npolicy = np.zeros([env.observation_space.n, env.action_space.n]) / env.action_space.n\nfor s in range(env.observation_space.n):\n    q = np.zeros(env.action_space.n)\n    for a in range(env.action_space.n):\n        for prob, next_state, reward, done in env.P[s][a]:\n            q[a] += prob * (reward + gamma * V[next_state])\n    best_a = np.argwhere(q==np.max(q)).flatten()\n    policy[s] = np.sum([np.eye(env.action_space.n)[i] for i in best_a], axis=0)/len(best_a)\nrld.plot_frozenlake(env=env, v=V, policy=policy, draw_vals=True)\n\n\n# ...the policy is note too bad after one greedy policy step, but it's not perfect - we really need to do control"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## On-Policy SARSA TD control\n\n- warning! sometimes this doesn't converge! run it a few times...\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def random_epsilon_greedy_policy(Q, epsilon, state, nA):\n    A = np.ones(nA, dtype=float) * epsilon / nA\n    best_action = np.argmax(Q[state])\n    A[best_action] += (1.0 - epsilon)\n    return A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Q = np.zeros([env.observation_space.n, env.action_space.n])\nnum_episodes = 10000\ngamma = 1.0\nepsilon = 0.4\nalpha = 0.2\nstats_rewards = defaultdict(float)\nstats_lengths = defaultdict(float)\n\nfor episode in range(1,num_episodes+1):\n    s, _ = env.reset()\n    p_a = random_epsilon_greedy_policy(Q, epsilon, s, env.action_space.n)\n    a = np.random.choice(np.arange(len(p_a)), p=p_a)\n    for t in itertools.count():\n        next_s, reward, term, trun, _ = env.step(a)\n        done = term or trun\n        p_next_a = random_epsilon_greedy_policy(Q,epsilon, next_s, env.action_space.n)\n        next_a   = np.random.choice(np.arange(len(p_next_a)), p=p_next_a)\n        Q[s][a] += alpha * (reward + gamma*Q[next_s][next_a] - Q[s][a])\n\n        stats_rewards[episode] += reward\n        stats_lengths[episode] = t\n        if done: break\n        s = next_s\n        a = next_a\n\n    if episode % 1000 == 0 and episode != 0:\n        print(f\"episode: {episode}/{num_episodes}\")\n\nprint(f\"episode: {episode}/{num_episodes}\")\n\nenv.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def moving_average(a, n=100) :\n    ret = np.cumsum(a, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n\n\nsmoothed_lengths = moving_average(np.array(list(stats_lengths.values())))\n\nplt.rcParams['figure.dpi'] = 150\n\nplt.figure(1, figsize=(3,3))\nplt.plot(smoothed_lengths)\nplt.xlabel('episode')\nplt.ylabel('episode length smoothed')\n\nsmoothed_rewards = moving_average(np.array(list(stats_rewards.values())))\nplt.figure(2, figsize=(3,3))\nplt.plot(smoothed_rewards)\nplt.xlabel('episode')\nplt.ylabel('reward (smoothed)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def pi_star_from_Q(Q):\n    done = False\n    pi_star = np.zeros([env.observation_space.n, env.action_space.n])\n    state, _ = env.reset() # start in top-left, = 0\n    while not done:\n        action = np.argmax(Q[state, :])\n        pi_star[state,action] = 1\n        state, reward, term, trun, _ = env.step(action)\n        done = term or trun\n    return pi_star"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rld.plot_frozenlake(env=env, policy=pi_star_from_Q(Q))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Off-policy Q-learning TD Control\n\n- warning! sometimes this doesn't converge! run it a few times...\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Q = np.zeros([env.observation_space.n, env.action_space.n])\nnum_episodes = 10000\ngamma = 1.0\nepsilon = 0.4\nalpha = 0.2\nstats_rewards = defaultdict(float)\nstats_lengths = defaultdict(float)\n\nfor episode in range(1,num_episodes+1):\n    s, _ = env.reset()\n    for t in itertools.count():\n        p_a = random_epsilon_greedy_policy(Q, epsilon, s, env.action_space.n)\n        a   = np.random.choice(np.arange(len(p_a)), p=p_a)\n        next_s, reward, term, trun, _ = env.step(a)\n        done = term or trun\n        Q[s][a] += alpha * (reward + gamma*np.max(Q[next_s][:]) - Q[s][a])\n\n        stats_rewards[episode] += reward\n        stats_lengths[episode] = t\n        if done: break\n        s = next_s\n\n    if episode % 1000 == 0 and episode != 0:\n        print(f\"episode: {episode}/{num_episodes}\")\n\nprint(f\"episode: {episode}/{num_episodes}\")\n\nenv.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# plot the results\nsmoothed_lengths = moving_average(np.array(list(stats_lengths.values())))\n\nplt.rcParams['figure.dpi'] = 150\n\nplt.figure(1, figsize=(3,3))\nplt.plot(smoothed_lengths)\nplt.xlabel('episode')\nplt.ylabel('episode length smoothed')\n\nsmoothed_rewards = moving_average(np.array(list(stats_rewards.values())))\nplt.figure(2, figsize=(3,3))\nplt.plot(smoothed_rewards)\nplt.xlabel('episode')\nplt.ylabel('reward (smoothed)')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}