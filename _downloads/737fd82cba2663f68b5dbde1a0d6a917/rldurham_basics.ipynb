{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Basics of the RLDurham package\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\nimport rldurham as rld  # Reinforcement Learning Durham package with helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple gym workflow\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# initialise environment\nenv = gym.make('CartPole-v1')\nobservation, info = env.reset(seed=42)\n\n# run episodes\nfor episode in range(10):\n    observation, info = env.reset()\n    done = False\n    while not done:\n        action = env.action_space.sample()  # random action\n        observation, reward, terminated, truncated, info = env.step(action)\n        done = terminated or truncated\n\n# close environment\nenv.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RLDurham workflow\nThis is the workflow required for the coursework to ensure correct logging\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# drop-in for gym.make that enables logging (use render_mode=\"rgb_array\" to enable video rendering)\nenv = rld.make('CartPole-v1', render_mode=\"rgb_array\")\n\n# record statistics (returned in info) and videos\nenv = rld.Recorder(\n    env,\n    smoothing=10,                       # track rolling averages (not required for coursework)\n    video=True,                         # enable recording videos\n    video_folder=\"videos\",              # folder for videos\n    video_prefix=\"xxxx00-agent-video\",  # prefix for videos\n    logs=True,                          # keep logs\n)\n\n# make reproducible by seeding everything (python, numpy, pytorch, env)\n# this also calls env.reset\nseed, observation, info = rld.seed_everything(42, env)\n\n# optionally track statistics for plotting\ntracker = rld.InfoTracker()\n\n# run episodes\nfor episode in range(11):\n    # recording statistics and video can be switched on and off (video recording is slow!)\n    env.info = episode % 2 == 0   # track every other episode (usually tracking every episode is fine)\n    env.video = episode % 4 == 0  # you only want to record videos every x episodes (set BEFORE calling reset!)\n    #######################################################################\n    # this is the same as above\n    observation, info = env.reset()\n    done = False\n    while not done:\n        action = env.action_space.sample()  # Random action\n        observation, reward, terminated, truncated, info = env.step(action)\n        done = terminated or truncated\n    #######################################################################\n        if done:\n            # per-episode statistics are returned by Recorder wrapper in info\n            #     'idx': index/count of the episode\n            #     'length': length of the episode\n            #     'r_sum': sum of rewards in episode\n            #     'r_mean': mean of rewards in episode\n            #     'r_std': standard deviation of rewards in episode\n            #     'length_': average `length' over smoothing window\n            #     'r_sum_': reward sum over smoothing window (not the average)\n            #     'r_mean_': average reward sum per episode over smoothing window (i.e. average of `r_sum')\n            #     'r_std_': standard deviation of reward sum per episode over smoothing window\n\n            # InfoTracker turns these into arrays over time\n            tracker.track(info)\n            print(tracker.info)\n\n            # some plotting functionality is provided (this will refresh in notebooks)\n            #  - combinations of ``r_mean`` and ``r_std`` or ``r_mean_`` and ``r_std_`` are most insightful\n            #  - for `CartPole`, ``length`` and ``r_sum`` are the same as there is a unit reward for each\n            #    time step of successful balancing\n            tracker.plot(r_mean_=True, r_std_=True,\n                         length=dict(linestyle='--', marker='o'),\n                         r_sum=dict(linestyle='', marker='x'))\n\n# don't forget to close environment (e.g. triggers last video save)\nenv.close()\n\n# write log file (for coursework)\nenv.write_log(folder=\"logs\", file=\"xxxx00-agent-log.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# print log file\nimport pandas as pd\nlog_file = \"logs/xxxx00-agent-log.txt\"\nprint(f\"log file: {log_file}\")\nprint(pd.read_csv(log_file, sep=\"\\t\").head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# show video\nimport os, re\nfrom ipywidgets import Video\nvideo_file = \"videos/\" + sorted(f for f in os.listdir(\"videos\") if re.match(r\".+episode=4.+\\.mp4\", f))[0]\nprint(f\"video file: {video_file}\")\nVideo.from_file(video_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Other Helper Functions\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is automatically run on package import to check for updates\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rld.version_check.check_for_update()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get information about simple action and observation spaces (only works for simple environments)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "discrete_act, discrete_obs, act_dim, obs_dim = rld.env_info(env, print_out=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Render the environment with matplotlib (essentially a single video frame; requires ``render_mode=\"rgb_array\"``)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rld.render(env)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}