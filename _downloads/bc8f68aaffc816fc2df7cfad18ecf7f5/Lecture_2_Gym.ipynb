{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Lecture 2: Gym\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# # Lecture 2: Gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dependencies and setup\n\n(this can take a minute or so...)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# !pip install swig\n\n# !pip install rldurham  # latest release\n# !pip install git+https://github.com/robert-lieck/rldurham.git@main  # latest main version (typically same as release)\n# !pip install git+https://github.com/robert-lieck/rldurham.git@dev  # latest dev version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\nimport rldurham as rld  # Reinforcement Learning Durham package with helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic environment\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v1', render_mode=\"human\")\nobservation, info = env.reset(seed=42)\n\nfor episode in range(10):\n    observation, info = env.reset()\n    done = False\n    while not done:\n        action = env.action_space.sample()  # random action\n        observation, reward, terminated, truncated, info = env.step(action)\n        done = terminated or truncated\n\nenv.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reinforcement Learning Durham:\n\n_rldurham_ Python package\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = rld.make('CartPole-v1', render_mode=\"rgb_array\")  # drop-in for gym.make\nenv = rld.Recorder(                                     # record statistics (returned in info) and videos\n    env, \n    smoothing=10,                      # rolling averages\n    video=True,                        # record videos\n    video_folder=\"videos\",             # folder for videos\n    video_prefix=\"xxxx00-agent-video\", # prefix for videos\n    logs=True,                         # keep logs\n)\nseed, observation, info = rld.seed_everything(42, env)  # seed everything (python, numpy, pytorch, env)\ntracker = rld.InfoTracker()                             # track statistics, e.g., for plotting\n\nfor episode in range(11):\n    env.info = episode % 2 == 0   # track every other episode\n    env.video = episode % 4 == 0  # set before reset! (is checked on reset)\n    #######################################################################\n    observation, info = env.reset()\n    done = False\n    while not done:\n        action = env.action_space.sample()  # Random action\n        observation, reward, terminated, truncated, info = env.step(action)\n        done = terminated or truncated\n    #######################################################################\n        if done:\n            # track and plot statistics\n            print(info)\n            print(tracker.info)\n            tracker.track(info)\n            tracker.plot(r_mean_=True, r_std_=True, \n                         length=dict(linestyle='--', marker='o'),\n                         r_sum=dict(linestyle='', marker='x'))\n\nenv.close()  # important (e.g. triggers last video save)\nenv.write_log(folder=\"logs\", file=\"xxxx00-agent-log.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Different environments\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "## render mode\n# rm = \"human\"     # for rendering in separate window\nrm = 'rgb_array' # for recording videos or rendering in notebook\n# rm = None        # no rendering\n\n## select environment\nenv = rld.make('CartPole-v1', render_mode=rm)              # easy discrete\n# env = rld.make('FrozenLake-v1', render_mode=rm)            # easy discrete\n# env = rld.make('LunarLander-v3', render_mode=rm)           # discrete\n# env = rld.make('ALE/Breakout-v5', render_mode=rm)          # discrete (atari)\n# env = rld.make('Pong-ram-v4', render_mode=rm)              # discrete (atari)\n# env = rld.make('Gravitar-ram-v4', render_mode=rm)          # hard discrete (atari)\n#\n# env = rld.make('Pendulum-v1', render_mode=rm)              # easy continuous\n# env = rld.make('LunarLanderContinuous-v3', render_mode=rm) # continuous\n# env = rld.make('BipedalWalker-v3', render_mode=rm)         # continuous\n# env = rld.make('BipedalWalkerHardcore-v3', render_mode=rm) # hard continuous\n\n## wrap for stats and video recording (requires render_mode='rgb_array')\n# env = rld.Recorder(env, video=True)\n# env.video = False  # deactivate\n\n# get some info\ndiscrete_act = hasattr(env.action_space, 'n')\ndiscrete_obs = hasattr(env.observation_space, 'n')\nact_dim = env.action_space.n if discrete_act else env.action_space.shape[0]\nobs_dim = env.observation_space.n if discrete_obs else env.observation_space.shape[0]\ndiscrete_act, discrete_obs, act_dim, obs_dim = rld.env_info(env)  # same\nprint(f\"The environment has \"\n      f\"{act_dim}{(f' discrete' if discrete_act else f'-dimensional continuous')} actions and \"\n      f\"{obs_dim}{(f' discrete' if discrete_obs else f'-dimensional continuous')} observations.\")\nprint('The maximum timesteps is: {}'.format(env.spec.max_episode_steps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for episode in range(1):\n    observation, info = env.reset()\n    rld.render(env)\n    done = False\n    for step in range(200):\n        action = env.action_space.sample()  # random action\n        observation, reward, terminated, truncated, info = env.step(action)\n        rld.render(env)\n        done = terminated or truncated\n        if done:\n            break\nenv.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training an agent\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport torch\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint('The device is: {}'.format(device))\nif device.type != 'cpu': print('It\\'s recommended to train on the cpu for this')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Agent(torch.nn.Module):\n    def __init__(self, env):\n        super().__init__()\n        self.discrete_act, self.discrete_obs, self.act_dim, self.obs_dim = rld.env_info(env)\n\n    def prob_action(self, obs):\n        return np.ones(self.act_dim)/self.act_dim\n\n    def sample_action(self, prob):\n        if self.discrete_act:\n            return np.random.choice(self.act_dim, p=prob)\n        else:\n            return np.random.uniform(-1.0, 1.0, size=self.act_dim)\n\n    def train(self):\n        return\n\n    def put_data(self, item):\n        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# init environment\nenv = rld.make('CartPole-v1')\nenv = rld.Recorder(env, smoothing=10)\n\n# seed\nrld.seed_everything(seed=42, env=env)\n\n# init agent\nagent = Agent(env)\n\n# training procedure\ntracker = rld.InfoTracker()\nfor episode in range(201):\n    obs, info = env.reset()\n    done = False\n\n    # get episode\n    while not done:\n        # select action\n        prob = agent.prob_action(obs)\n        action = agent.sample_action(prob)\n\n        # take action in environment\n        next_obs, reward, terminated, truncated, info = env.step(action)\n        done = terminated or truncated\n        agent.put_data((reward, prob[action] if agent.discrete_act else None))\n        obs = next_obs\n\n    # track and plot\n    tracker.track(info)\n    if episode % 10 == 0:\n        tracker.plot(r_mean_=True, r_std_=True)\n\n    # update agent's policy\n    agent.train()\nenv.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## REINFORCE agent example\n\n\nThis code is based on: https://github.com/seungeunrho/minimalRL\n\nNote: these implementations are good to study, although most are for discrete action spaces\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# this is an implementation of REINFORCE (taught in lecture 8) - one of the simplest classical policy gradient methods\n# this will only work for simple discrete control problems like cart pole or (slowly) lunar lander discrete\nplot_interval = 50\nvideo_every = 500\nmax_episodes = 5000\n\n\nclass Agent(torch.nn.Module):\n    def __init__(self, env):\n        super().__init__()\n        discrete_act, discrete_obs, act_dim, obs_dim = rld.env_info(env)\n        if not discrete_act:\n            raise RuntimeError(\"REINFORCE only works for discrete action spaces\")\n        self.data = []\n        self.fc1 = torch.nn.Linear(obs_dim, 128)\n        self.fc2 = torch.nn.Linear(128, act_dim)\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0002)\n\n    def sample_action(self, prob):\n        m = torch.Categorical(prob)\n        a = m.sample()\n        return a.item()\n\n    def prob_action(self, s):\n        x = torch.F.relu(self.fc1(torch.from_numpy(s).float()))\n        return torch.F.softmax(self.fc2(x), dim=0)\n\n    def put_data(self, item):\n        self.data.append(item)\n\n    def train(self):\n        R = 0\n        self.optimizer.zero_grad()\n        for r, prob in self.data[::-1]:\n            R = r + 0.98 * R\n            loss = -torch.log(prob) * R\n            loss.backward()\n        self.optimizer.step()\n        self.data = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom environments (here: multi-armed bandits)\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# one armed bandits are slot machines where you can win or loose money\n# some casinos tune them, so some machines are less successful (the probability distribution of winning)\n# the machines also dish out varying rewards (the reward distribution)\n# so basically each machine has some probability of dishing out \u00a3 reward (p_dist doesn't need to sum to 1) else it gives \u00a30\n\nclass BanditEnv(gym.Env):\n\n    def __init__(self, p_dist=[0.4,0.2,0.1,0.1,0.1,0.7], r_dist=[1,0.1,2,0.5,6,70]):\n\n        self.p_dist = p_dist\n        self.r_dist = r_dist\n\n        self.n_bandits = len(p_dist)\n        self.action_space = gym.spaces.Discrete(self.n_bandits)\n        self.observation_space = gym.spaces.Discrete(1)\n\n    def step(self, action):\n        assert self.action_space.contains(action)\n\n        reward = -25\n        terminated = True\n        truncated = False\n\n        if np.random.uniform() < self.p_dist[action]:\n            if not isinstance(self.r_dist[action], list):\n                reward += self.r_dist[action]\n            else:\n                reward += np.random.normal(self.r_dist[action][0], self.r_dist[action][1])\n\n        return np.zeros(1), reward, terminated, truncated, {}\n\n    def reset(self, seed=None, options=None):\n        if seed is not None:\n            self.np_random, seed = gym.utils.seeding.np_random(seed)\n        return np.zeros(1), {}\n\n    def render(self):\n        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# initialise\ndiscrete = True\nenv = BanditEnv()\nobs_dim = 1\nact_dim = len(env.p_dist)\nenv.spec = gym.envs.registration.EnvSpec('BanditEnv-v0', max_episode_steps=5)\nmax_episodes = 1000\n\nfor episode in range(10):\n    observation, info = env.reset()\n    done = False\n    while not done:\n        action = env.action_space.sample()  # random action\n        observation, reward, terminated, truncated, info = env.step(action)\n        print(action, reward)\n        done = terminated or truncated\n\nenv.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}