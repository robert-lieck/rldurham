{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Lecture 5: Monte Carlo Methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# # Lecture 5: Monte Carlo Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport rldurham as rld\n\n\n# ## Learning a Policy with Monte Carlo Sampling\n\n# Our goal is to learn an optimal policy from randomly sampled trajectories. Our strategy is to estimate Q-values (state-action values) based on the samples and get the policy from the Q-values.\n\n# ### Essential Components\n\n# We can **define the policy** based on the Q-values by either deterministically picking an action (not a good idea) or giving equal probability to all actions with maximum value. Additionally, we can add uniform random actions with probability epsilon (exploration)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy(Q, epsilon, deterministic):\n    p = np.zeros_like(Q)\n    ns, na = Q.shape\n    for s in range(ns):\n        Qs = Q[s]\n        if deterministic:\n            max_action = np.argmax(Qs)\n            p[s, max_action] = 1\n        else:\n            max_actions = np.argwhere(Qs == Qs.max())\n            p[s, max_actions] = 1 / len(max_actions)\n        p[s] = (1 - epsilon) * p[s] + epsilon / na\n    return p\n\n\n# Given a policy, we can **sample episodes** in the environment, that is, complete trajectories that reach the goal state (or run over the time limit)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def sample_episode(env, policy):\n    observation, info = env.reset()\n    done = False\n    trajectory = []\n    while not done:\n        action = np.random.choice(env.action_space.n, p=policy[observation])\n        new_observation, reward, term, trunc, info = env.step(action)\n        trajectory.append((observation, action, reward))\n        observation = new_observation\n        done = term or trunc\n    return trajectory, info\n\n\n# From the trajectory, we can **compute returns** (discounted cumulative rewards) for each state along the way, which is most efficiently done in reverse order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_returns(trajectory, gamma):\n    partial_return = 0.\n    returns = []\n    for observation, action, reward in reversed(trajectory):\n        partial_return *= gamma\n        partial_return += reward\n        returns.append((observation, action, partial_return))\n    return list(reversed(returns))\n\n\n# Frome the returns, we can now **update the Q-values** using empirical averages as a Monte Carlo approximation of the expected return. This can be done using exact averages or exponentially smoothing averages (with constant learning rate alpha)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def update_Q(Q, ns, returns, alpha):\n    for obs, act, ret in returns:\n        ns[obs, act] += 1             # update counts\n        if alpha is None:\n            alpha = 1 / ns[obs, act]  # use exact means if no learning rate provided\n            Q[obs, act] += alpha * (ret - Q[obs, act])\n        else:\n            old_bias_correction = 1 - (1 - alpha) ** (ns[obs, act] - 1)\n            new_bias_correction = 1 - (1 - alpha) ** ns[obs, act]\n            Q[obs, act] = Q[obs, act] * old_bias_correction  # undo old bias correction\n            Q[obs, act] += alpha * (ret - Q[obs, act])       # normal update as above\n            Q[obs, act] = Q[obs, act] / new_bias_correction  # apply new bias correction\n\n\n# ### Some Examples\n\n# Let's look at different scenarios starting with an empty lake and going through different hyper-parameter settings:\n#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Empty Lake\n\n\n\n1. **epsilon, gamma, det, alpha = 0.0, 1.0, True,  None**\n\n   A deterministic policy without exploration typically does not learn at all because it never reaches the goal state.\n\n2. **epsilon, gamma, det, alpha = 0.0, 1.0, False, None**\n\n   A non-deterministic policy without exploration samples a successful episode at some point but then \"clings\" to it without exploring further, so is likely to get stuck and never find the optimal policy.\n\n4. **epsilon, gamma, det, alpha = 0.1, 1.0, False, None**\n\n   A little exploration produces much more stable results and will eventually find the optimal policy. Without any discount it will not have a preference to shorter (or even finite) paths.\n\n5. **epsilon, gamma, det, alpha = 0.5, 0.9, False, None**\n\n   Considerable exploration and some discount produces very stable results with a preference for shorter paths, but the policy is far from optimal due to exploration.\n\n%%\n8x8 Lake\n--------\n\n\n\n- Things are more difficult because there are more \"pockets\" to explore.\n\n%%\nExploration Noise\n-----------------\n\n\n\n- Run **epsilon, gamma, det, alpha = 0.3, 1.0, False, None** on small custom environment (`slippery=True`) for 1000 episodes\n    - Currently optimal policy takes the short-but-risky path because everything is also risky with exploration noise.\n- Switch to **epsilon, alpha = 0.2, 0.01** and run for another 2000 episodes\n    - Now the long-but-safe path is preferred as it should be (with gamma=1)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# set up environment\nenv = rld.make(\n    'FrozenLake-v1',     # simple\n    # 'FrozenLake8x8-v1',  # more complex\n    desc = [             # empty lake (start with this as it is most insightful)\n        \"SFFFFFFF\",\n        \"FFFFFFFF\",\n        \"FFFFFFFF\",\n        \"FFFFFFFF\",\n        \"FFFFFFFF\",\n        \"FFFFFFFF\",\n        \"FFFFFFFF\",\n        \"FFFFFFFG\",\n    ],\n    is_slippery=False,\n    # desc=[               # short high-risk versus long low-risk paths with is_slippery=True\n    #     \"FFF\",\n    #     \"FHF\",\n    #     \"SFG\",\n    #     \"FHF\",\n    # ],\n    # is_slippery=True,\n    render_mode=\"rgb_array\", \n    )\nLEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\nenv = rld.Recorder(env, smoothing=100)\ntracker = rld.InfoTracker()\nrld.seed_everything(42, env)\nrld.render(env)\n\n# initialise Q values\nQ = np.zeros((env.observation_space.n, env.action_space.n))\nns = np.zeros((env.observation_space.n, env.action_space.n), dtype=int)\n\n# different hyper parameters\nepsilon, gamma, det, alpha = 0.0, 1.0, True,  None  # does not learn at all\n# epsilon, gamma, det, alpha = 0.0, 1.0, False, None  # very instable and gets stuck quickly\n# epsilon, gamma, det, alpha = 0.1, 1.0, False, None  # more stable but no preference for shorter paths\n# epsilon, gamma, det, alpha = 0.5, 0.9, False, None  # stable and preference for shorter paths, but non-optimal policy\n# epsilon, gamma, det, alpha = 0.3, 1.0, False, None  # sub-optimal policy due to exploration noise (on small custom map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sample episodes\n# n_episodes, plot_every = 1, 1       # one trial at a time\nn_episodes, plot_every = 1000, 100  # many trials at once\n# epsilon = 0.                        # force optimal policy\n# epsilon, alpha = 0.2, 0.01          # less exploration, some forgetting\nfor eidx in range(n_episodes):\n    # epsilon-greedy policy\n    policy = epsilon_greedy_policy(Q=Q, epsilon=epsilon, deterministic=det)\n    \n    # sample complete episode\n    trajectory, info = sample_episode(env=env, policy=policy)\n    \n    # compute step-wise returns from trajectory\n    returns = compute_returns(trajectory=trajectory, gamma=gamma)\n    \n    # update Q values\n    update_Q(Q=Q, ns=ns, returns=returns, alpha=alpha)\n\n    # track and plot progress\n    tracker.track(info)\n    if (eidx + 1) % plot_every == 0:\n        tracker.plot(r_sum=True, r_mean_=True, clear=True)\n        rld.plot_frozenlake(env, v=Q.max(axis=1), \n                            policy=epsilon_greedy_policy(Q=Q, epsilon=epsilon, deterministic=det), \n                            trajectory=trajectory, draw_vals=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\nprint(\"First steps (state, action, reward):\\n\", trajectory[:3])\nprint(\"First returns (state, action, return):\\n\", returns[:3])\nprint(\"Q values for first states:\\n\", Q[:3])\nprint(\"Action counts for first states:\\n\", ns[:3])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}