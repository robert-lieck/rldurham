{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Lecture 4: Dynamic Programming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# # Lecture 4: Dynamic Programming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom scipy.special import softmax\nimport rldurham as rld"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = rld.make(\n    'FrozenLake-v1',         # import the frozen lake environment\n    # 'FrozenLake8x8-v1',    # use a bigger version\n    render_mode=\"rgb_array\", # for rendering as image/video\n    is_slippery=False,       # warning: slippery=True results in very complex environment dynamics where the optimal solution is not very intuitive to humans!\n    # desc=[\"GFFS\", \"FHFH\", \"FFFH\", \"HFFG\"],  # define custom map\n)\nrld.seed_everything(42, env)\n# some info\nrld.env_info(env, print_out=True)\nprint('action space: ' + str(env.action_space))\nprint('observation space: ' + str(env.observation_space))\nrld.render(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# actions\nLEFT, DOWN, RIGHT, UP = 0,1,2,3\n\n# lets do an example step for the policy\nenv.reset()\nnext_state, reward, term, trunc, info = env.step(RIGHT)\nprint('=============')\nprint('next state: ' + str(next_state))\nprint('terminated: ' + str(term))\nprint('truncated: ' + str(trunc))\nprint('    reward: ' + str(reward))\nprint('      info: ' + str(info))\nrld.render(env, clear=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Policy evaluation\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(env, policy, gamma=1, theta=1e-8, draw=False):\n    V = np.zeros(env.observation_space.n)\n    while True:\n        delta = 0\n        for s in range(env.observation_space.n):\n            Vs = 0\n            for a, action_prob in enumerate(policy[s]):\n                for prob, next_state, reward, done in env.P[s][a]:\n                    Vs += action_prob * prob * (reward + gamma * V[next_state])\n            delta = max(delta, np.abs(V[s]-Vs))\n            V[s] = Vs\n        if draw:\n            rld.plot_frozenlake(env=env, v=V, policy=policy, draw_vals=True, clear=True)\n        if delta < theta:\n            break\n    return V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# lets start with a random policy, in this case there's a 1/4 probability of taking any action at every 4x4 state\npolicy = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# evaluate this policy (change draw=True to show steps, and ensure environment is 'FrozenLake-v1' for the exact same steps in the lecture)\nV = policy_evaluation(env, policy, draw=True)\n\n\n# Get $q_\\pi$ form $v_\\pi$ by a one-step look ahead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def q_from_v(env, V, s, gamma=1):\n    q = np.zeros(env.action_space.n)\n    for a in range(env.action_space.n):\n        for prob, next_state, reward, done in env.P[s][a]:\n            q[a] += prob * (reward + gamma * V[next_state])\n    return q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def policy_improvement(env, V, gamma=1):\n    policy = np.zeros([env.observation_space.n, env.action_space.n]) / env.action_space.n\n    for s in range(env.observation_space.n):\n        q = q_from_v(env, V, s, gamma)\n\n        # # deterministic policy (will always choose one specific an action and does not capture the distribution)\n        # policy[s][np.argmax(q)] = 1\n\n        # stochastic optimal policy (puts equal probability on all maximizing actions)\n        best_a = np.argwhere(q==np.max(q)).flatten()\n        policy[s] = np.sum([np.eye(env.action_space.n)[i] for i in best_a], axis=0) / len(best_a)\n\n        # # softmax policy that adds some exploration\n        # policy[s] = softmax(q / 0.01)\n\n    return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# plot the policy from a single greedy improvement step after policy evaluation\npolicy = policy_improvement(env, V)\nrld.plot_frozenlake(env, V, policy, draw_vals=True)\nrld.plot_frozenlake(env, V, policy, draw_vals=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# lets from here on use a larger grid world\nenv = rld.make('FrozenLake8x8-v1', is_slippery=False)\nrld.seed_everything(42, env)\npolicy = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n\nV = policy_evaluation(env, policy, draw=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# show improved policy from the policy evaluation, for the 8x8 case it's still not great\nnew_policy = policy_improvement(env, V)\nrld.plot_frozenlake(env, V, new_policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# now solve the MDP by policy iteration\ndef policy_iteration(env, gamma=1, theta=1e-8):\n    policy = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n\n    while True:\n\n        # evaluate the policy (get the value function)\n        V = policy_evaluation(env, policy, gamma, theta)\n\n        # greedily choose the best action\n        new_policy = policy_improvement(env, V)\n\n        if np.max(abs(policy_evaluation(env, policy) - policy_evaluation(env, new_policy))) < theta*1e2:\n           break;\n\n        policy = new_policy.copy()\n    return policy, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# do policy iteration\npolicy_pi, V_pi = policy_iteration(env, gamma=0.7)\nrld.plot_frozenlake(env, V_pi, policy_pi, draw_vals=True)\nrld.plot_frozenlake(env, V_pi, policy_pi, draw_vals=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# now lets do value iteration, which is the k=1 case but simplifies saving computation\n# note how there are no intermediate policies until the end\ndef value_iteration(env, gamma=1, theta=1e-8):\n    V = np.zeros(env.observation_space.n) # initial state value function\n    while True:\n        delta = 0\n        for s in range(env.observation_space.n):\n            v_s = V[s] # store old value\n            q_s = q_from_v(env, V, s, gamma) # the action value function is calculated for all actions\n            V[s] = max(q_s) # the next value of the state function is the maximum of all action values\n            delta = max(delta, abs(V[s] - v_s))\n        if delta < theta: break\n    # lastly, at convergence, we can get a (optimal) policy from the optimal state value function\n    policy = policy_improvement(env, V, gamma)\n    return policy, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "policy_pi, V_pi = value_iteration(env, gamma=0.7)\nrld.plot_frozenlake(env, V_pi, policy_pi, draw_vals=True)\nrld.plot_frozenlake(env, V_pi, policy_pi, draw_vals=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# here's an expanded version of the above that's more similar to lecture slides\ndef value_iteration(env, gamma=1, theta=1e-8):\n    V = np.zeros(env.observation_space.n)\n    while True:\n        delta = 0\n        for s in range(env.observation_space.n):\n            v_s = V[s]\n\n            # one step look ahead to get q from v\n            q_s = np.zeros(env.action_space.n)\n            for a in range(env.action_space.n):\n                for prob, next_state, reward, done in env.P[s][a]:\n                    q_s[a] += prob * (reward + gamma * V[next_state])\n\n            V[s] = max(q_s)\n            delta = max(delta, abs(V[s] - v_s))\n        if delta < theta: break\n\n    policy = policy_improvement(env, V, gamma)\n    return policy, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "policy_pi, V_pi = value_iteration(env, gamma=0.7)\nrld.plot_frozenlake(env, V_pi, policy_pi, draw_vals=True)\nrld.plot_frozenlake(env, V_pi, policy_pi, draw_vals=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}