{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Practical 6: Temporal Difference Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# # Practical 6: Temporal Difference Learning\n\n# ## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\nfrom IPython import display\nimport rldurham as rld\nfrom rldurham import plot_frozenlake as plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "name = 'FrozenLake-v1'\nenv = rld.make(name, is_slippery=False)                 # 4x4\nenv = rld.make(name, map_name=\"8x8\", is_slippery=False) # 8x8\n# env = rld.make(name, desc=[\"SFHH\",\n#                            \"HFFH\",\n#                            \"HHFF\",\n#                            \"HHHG\",], is_slippery=False) # custom\nrld.seed_everything(42, env)\n# LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n\n\n# You can use this helper class to define epsilon-greed policies based on $Q$-values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class QPolicy:\n    def __init__(self, Q, epsilon):\n        self.Q = Q\n        self.epsilon = epsilon\n        \n    def sample(self, state):\n        return np.random.choice(np.arange(self.Q.shape[1]), p=self[state])\n        if np.random.rand() > self.epsilon:\n            best_actions = np.argwhere(self.Q[state]==np.max(self.Q[state])).flatten()\n            return np.random.choice(best_actions)\n        else:\n            return env.action_space.sample()\n\n    def __getitem__(self, state):\n        Qs = self.Q[state]\n        p = np.zeros_like(Qs)\n        max_actions = np.argwhere(Qs == Qs.max())\n        p[max_actions] = 1 / len(max_actions)\n        return (1 - self.epsilon) * p + self.epsilon / len(p)\n\n\n# We can keep some plotting data in these variables (re-evaluate the cell to clear data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reward_list = [[]]\nauc = [0]\ntest_reward_list = [[]]\ntest_auc = [0]\nplot_data = [[]]\nplot_labels = []\nexperiment_id = 0\n\n\n# and use these functions to update and plot the learning progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# (using global variables in functions)\ndef update_plot(mod):\n    reward_list[experiment_id].append(reward_sum)\n    auc[experiment_id] += reward_sum\n    test_reward_list[experiment_id].append(test_reward_sum)\n    test_auc[experiment_id] += test_reward_sum\n    if episode % mod == 0:\n        plot_data[experiment_id].append([episode,\n                                         np.array(reward_list[experiment_id]).mean(),\n                                         np.array(test_reward_list[experiment_id]).mean()])\n        reward_list[experiment_id] = []\n        test_reward_list[experiment_id] = []\n        for i in range(len(plot_data)):\n            lines = plt.plot([x[0] for x in plot_data[i]],\n                             [x[1] for x in plot_data[i]], '-', \n                             label=f\"{plot_labels[i]}, AUC: {auc[i]}|{test_auc[i]}\")\n            color = lines[0].get_color()\n            plt.plot([x[0] for x in plot_data[i]],\n                     [x[2] for x in plot_data[i]], '--', color=color)\n        plt.xlabel('Episode number')\n        plt.ylabel('Episode reward')\n        plt.legend()\n        display.clear_output(wait=True)\n        plt.show()\n        \ndef next_experiment():\n    reward_list.append([])\n    auc.append(0)\n    test_reward_list.append([])\n    test_auc.append(0)\n    plot_data.append([])\n    return experiment_id + 1\n\n\n# ## On-policy (SARSA) and off-policy (Q-Learning) with TD(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# parameters\nnum_episodes = 3000\nalpha = 0.1\ngamma = 0.9\nepsilon = 0.5\non_policy = True # SARSA or Q-Learning\n\n# value initialisation\nQ = np.random.uniform(0, 1e-5, [env.observation_space.n, env.action_space.n]) # noisy\nQ = np.zeros([env.observation_space.n, env.action_space.n])                   # neutral\nV = np.zeros([env.observation_space.n])\n\nif on_policy:\n    # policies for SARSA\n    # vvvvvvvvvvvvvvvvvv\n    sample_policy = QPolicy(Q, epsilon)\n    learned_policy = sample_policy\n    plot_labels.append(f\"SARSA (alpha={alpha}, epsilon={epsilon})\")\n    # ^^^^^^^^^^^^^^^^^^\nelse:\n    # policies for Q-Learning\n    # vvvvvvvvvvvvvvvvvvvvvvv\n    sample_policy = QPolicy(Q, epsilon)\n    td_epsilon = 0.01\n    learned_policy = QPolicy(Q, td_epsilon)\n    plot_labels.append(f\"Q-Learning (alpha={alpha}, epsilon={epsilon}|{td_epsilon})\")\n    # ^^^^^^^^^^^^^^^^^^^^^^^\n\nfor episode in range(num_episodes):\n    state, _ = env.reset()\n    reward_sum = 0\n    # learning a policy\n    for t in itertools.count():\n        action = sample_policy.sample(state)\n        next_state, reward, term, trun, _ = env.step(action)\n        done = term or trun\n        next_action = learned_policy.sample(next_state)\n        \n        # TD(0) targets\n        # vvvvvvvvvvvvv\n        v_target = reward + gamma * V[next_state]\n        q_target = reward + gamma * Q[next_state, next_action]\n        # ^^^^^^^^^^^^^\n        \n        # expected TD(0) target\n        # vvvvvvvvvvvvv\n        expected_Q = (learned_policy[next_state] * Q[next_state]).sum()\n        q_target = reward + gamma * expected_Q\n        # ^^^^^^^^^^^^^\n        \n        # updates\n        # vvvvvvvvvvvvv\n        s, a = state, action\n        V[s] += alpha * (v_target - V[s])\n        Q[s, a] += alpha * (q_target - Q[s, a])\n        # ^^^^^^^^^^^^^\n\n        reward_sum += reward\n        if done:\n            break\n        state = next_state\n\n    # testing the learned policy\n    state, _ = env.reset()\n    test_reward_sum = 0\n    while True:\n        action = learned_policy.sample(state)\n        next_state, reward, term, trun, _ = env.step(action)\n        done = term or trun\n        test_reward_sum += reward\n        state = next_state\n        if done:\n            break\n\n    update_plot(int(np.ceil(num_episodes / 20)))\n\nexperiment_id = next_experiment()\nprint(\"Sampling policy and values\")\nplot(env, v=V, policy=sample_policy, draw_vals=True)\nprint(\"Learned policy and optimal/max values\")\nplot(env, v=Q.max(axis=1), policy=learned_policy, draw_vals=True)\n\n\n# ## Multi-Step Targets with TD(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# parameters\nnum_episodes = 3000\nalpha = 0.1\ngamma = 0.9\nepsilon = 0.5\non_policy = True  # SARSA or Q-Learning\nn = 10            # length of trace to use\n\n# value initialisation\nQ = np.random.uniform(0, 1e-5, [env.observation_space.n, env.action_space.n]) # noisy\nQ = np.zeros([env.observation_space.n, env.action_space.n])                   # neutral\nV = np.zeros([env.observation_space.n])\n\nif on_policy:\n    # policies for SARSA\n    # vvvvvvvvvvvvvvvvvv\n    sample_policy = QPolicy(Q, epsilon)\n    learned_policy = sample_policy\n    plot_labels.append(f\"SARSA (n={n}, alpha={alpha}, epsilon={epsilon})\")\n    # ^^^^^^^^^^^^^^^^^^\nelse:\n    # policies for Q-Learning\n    # vvvvvvvvvvvvvvvvvvvvvvv\n    sample_policy = QPolicy(Q, epsilon)\n    td_epsilon = 0.01\n    learned_policy = QPolicy(Q, td_epsilon)\n    plot_labels.append(f\"Q-Learning (n={n}, alpha={alpha}, epsilon={epsilon}|{td_epsilon})\")\n    # ^^^^^^^^^^^^^^^^^^^^^^^\n\nfor episode in range(num_episodes):\n    state, _ = env.reset()\n    reward_sum = 0\n    done_n = 0\n\n    # trace of the last n + 1 transitions (state, action, reward, next_action)\n    trace = np.zeros((n + 1, 4), dtype=int) \n\n    # learning a policy\n    for t in itertools.count():\n        action = sample_policy.sample(state)\n        next_state, reward, term, trun, _ = env.step(action)\n        done = term or trun\n        next_action = learned_policy.sample(next_state)\n\n        # remember transitions (incl. next action sampled by learned policy)\n        trace[-1] = (state, action, reward, next_action)\n\n        # start computing updates if trace is long enough\n        if t > n:\n\n            # n-step targets\n            # vvvvvvvvvvvvvv\n            n_step_return = sum(gamma ** i * r for i, (_, _, r, _) in enumerate(trace))\n            v_target = n_step_return + gamma ** (n + 1) * V[next_state]\n            q_target = n_step_return + gamma ** (n + 1) * Q[next_state, next_action]\n            # ^^^^^^^^^^^^^^\n\n            # importance sampling factor for TD(n) Q-Learning\n            if on_policy:\n                rho = 1\n            else:\n                # vvvvvvvvvvvvvvvvvvvvvvvvvv\n                rho = np.prod([learned_policy[s][a] / sample_policy[s][a] for s, a, _, _ in trace])\n                # ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n            # updates\n            # vvvvvvv\n            s, a, _, _ = trace[0]\n            V[s] += alpha * rho * (v_target - V[s])\n            Q[s, a] += alpha * rho * (q_target - Q[s, a])\n            # ^^^^^^^\n\n        reward_sum += reward\n        state = next_state\n\n        # roll trace to make space for next transition at the end\n        trace = np.roll(trace, shift=-1, axis=0)\n\n        # fill with dummy transitions so we can learn from end of episode\n        done_n += done\n        if done_n > n:\n            break\n\n    # testing the learned policy\n    state, _ = env.reset()\n    test_reward_sum = 0\n    while True:\n        action = learned_policy.sample(state)\n        next_state, reward, term, trun, _ = env.step(action)\n        done = term or trun\n        test_reward_sum += reward\n        state = next_state\n        if done:\n            break\n\n    update_plot(int(np.ceil(num_episodes / 20)))\n\nexperiment_id = next_experiment()\nprint(\"Sampling policy and values\")\nplot(env, v=V, policy=sample_policy, draw_vals=True)\nprint(\"Learned policy and optimal/max values\")\nplot(env, v=Q.max(axis=1), policy=learned_policy, draw_vals=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}