{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Practical 4: DynamicProgramming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# # Practical 4: DynamicProgramming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport rldurham as rld"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "name = 'FrozenLake-v1'  # small version\n# name = 'FrozenLake8x8-v1'  # larger version\nenv = rld.make(name, is_slippery=False)\nrld.seed_everything(42, env)\nLEFT, DOWN, RIGHT, UP = 0, 1, 2, 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('action space: ' + str(env.action_space))\nprint('reward range: ' + str(env.reward_range))\nprint('observation space: ' + str(env.observation_space))\nrld.plot_frozenlake(env=env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def uniform_policy(env):\n    return np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\nrld.plot_frozenlake(env=env, policy=uniform_policy(env))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def policy_eval_step(env, policy, gamma, v_init=None):\n    if v_init is None:\n        v_init = np.zeros(env.observation_space.n)\n    v = np.zeros(env.observation_space.n)\n    for s_from in range(env.observation_space.n):\n        for a in range(env.action_space.n):\n            pi = policy[s_from, a]\n            for p, s_to, r, done in env.P[s_from][a]:\n                v[s_from] += pi * p * (r + gamma * v_init[s_to])\n    return v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "v = np.zeros(env.observation_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "v = policy_eval_step(env, uniform_policy(env), 1, v)\nrld.plot_frozenlake(env, v, uniform_policy(env), draw_vals=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def policy_eval_step_inplace(env, policy, gamma, v_init=None):\n    if v_init is None:\n        v_init = np.zeros(env.observation_space.n)\n    v = v_init.copy() # opearate on copy in-place\n    for s_from in reversed(range(env.observation_space.n)):  # reverse order of states\n        v_s_from = 0  # compute value for this state\n        for a in range(env.action_space.n):\n            pi = policy[s_from, a]\n            for p, s_to, r, done in env.P[s_from][a]:\n                v_s_from += pi * p * (r + gamma * v[s_to])  # use the values we also update\n        v[s_from] = v_s_from  # update\n    return v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(env, policy, gamma, v_init=None,\n                      print_iter=False, atol=1e-8, max_iter=10**10):\n    if v_init is None:\n        v_init = np.zeros(env.observation_space.n)\n    v = v_init\n    for i in range(1, max_iter + 1):\n        new_v = policy_eval_step(env, policy, gamma, v)\n        if np.allclose(v, new_v, atol=atol):\n            break\n        v = new_v\n    if print_iter:\n        print(f\"{i} iterations\")\n    return v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def q_from_v(env, v, s, gamma):\n    q = np.zeros(env.action_space.n)\n    for a in range(env.action_space.n):\n        for p, s_to, r, done in env.P[s][a]:\n            q[a] += p * (r + gamma * v[s_to])\n    return q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def policy_improvement(env, v, gamma, deterministic=False):\n    policy = np.zeros([env.observation_space.n, env.action_space.n]) / env.action_space.n\n    for s in range(env.observation_space.n):\n        q = q_from_v(env, v, s, gamma)\n        if deterministic:\n            # deterministic policy\n            policy[s][np.argmax(q)] = 1\n        else:\n            # stochastic policy with equal probability on maximizing actions\n            best_a = np.argwhere(q==np.max(q)).flatten()\n            policy[s, best_a] = 1 / len(best_a)\n    return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = rld.make('FrozenLake8x8-v1', is_slippery=False)\nrld.seed_everything(42, env)\npolicy = uniform_policy(env)\nv = policy_evaluation(env, policy, gamma=1)\nrld.plot_frozenlake(env, v, policy, draw_vals=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_policy = policy_improvement(env, v, gamma=1)\nrld.plot_frozenlake(env, v, new_policy, draw_vals=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gamma = 1\nv = policy_evaluation(env, new_policy, gamma=gamma)\nrld.plot_frozenlake(env, v=v, policy=new_policy, draw_vals=True)\nprint(v)\nnew_policy = policy_improvement(env, v, gamma=gamma)\nrld.plot_frozenlake(env, v=v, policy=new_policy, draw_vals=True)\n\n\n# In[ ]:"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}