{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Practical 3: Markov Decision Processes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# # Practical 3: Markov Decision Processes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n\n\n# ## Markov Chain\n\n# Define a transition matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_states = 10\nidx = np.arange(n_states)\ntransition_matrix = np.zeros((n_states, n_states))\ntransition_matrix[idx, (idx - 1) % n_states] = 0.1\ntransition_matrix[idx, (idx + 1) % n_states] = 0.9\ntransition_matrix[0, 0] = 0.8\ntransition_matrix[0, 1] = 0.1\ntransition_matrix[0, -1] = 0.1\nprint(f\"normalised: {np.all(transition_matrix.sum(axis=1)==1)}\")\n\n\n# Sample and plot episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def sample_episodes(n_episodes, length_episodes):\n    episodes = np.zeros((n_episodes, length_episodes), dtype=int)\n    for idx_episodes in range(n_episodes):\n        for idx_time in range(1, length_episodes):\n            current_state = episodes[idx_episodes, idx_time - 1]\n            probs = transition_matrix[current_state]\n            next_state = np.random.choice(n_states, p=probs)\n            episodes[idx_episodes, idx_time] = next_state\n    return episodes\n\nepisodes = sample_episodes(n_episodes=10, length_episodes=100)\nfig, ax = plt.subplots(1, 1, figsize=(15, 10))\nax.plot(np.arange(episodes.shape[1]), episodes.T, '-o', alpha=0.3);\n\n\n# Plot statistics of state visits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "episodes = sample_episodes(n_episodes=100, length_episodes=100)\nplt.hist((episodes[:, :50].flatten(), episodes[:, 50:].flatten()),\n         density=True,\n         label=[\"first 50 steps\", \"second 50 steps\"])\nplt.legend()\n\n\n# Compute the closed-form solution for stationary distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "eigvals, eigvecs = np.linalg.eig(transition_matrix.T)\nstationary = None\nfor idx in range(n_states):\n    if np.isclose(eigvals[idx], 1):\n        stationary = eigvecs[:, idx].real\n        stationary /= stationary.sum()\nprint(f\"Is stationary: {np.all(np.isclose(stationary @ transition_matrix, stationary))}\")\nprint(stationary)\n\n\n# ## Markov Reward Process\n\n# Add a reward function and estimate the expected reward over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def reward_function(s):\n    return int(s == 0)\n\ndef sample_episodes(n_episodes, length_episodes):\n    episodes = np.zeros((n_episodes, length_episodes, 2), dtype=int)\n    for idx_episodes in range(n_episodes):\n        for idx_time in range(1, length_episodes):\n            current_state = episodes[idx_episodes, idx_time - 1, 0]\n            probs = transition_matrix[current_state]\n            next_state = np.random.choice(n_states, p=probs)\n            episodes[idx_episodes, idx_time, 0] = next_state\n            episodes[idx_episodes, idx_time, 1] = reward_function(next_state)\n    return episodes\n\nepisodes = sample_episodes(n_episodes=1000, length_episodes=100)\nfig, ax = plt.subplots(1, 1, figsize=(15, 10))\nax.plot(np.arange(episodes.shape[1]), episodes[:, :, 1].mean(axis=0), '-o', alpha=0.3);\n\n\n# Add a discount factor and estimate state values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def sample_episodes(n_episodes, length_episodes, start_state):\n    episodes = np.zeros((n_episodes, length_episodes, 2), dtype=int)\n    episodes[:, 0, 0] = start_state\n    for idx_episodes in range(n_episodes):\n        for idx_time in range(1, length_episodes):\n            current_state = episodes[idx_episodes, idx_time - 1, 0]\n            probs = transition_matrix[current_state]\n            next_state = np.random.choice(n_states, p=probs)\n            episodes[idx_episodes, idx_time, 0] = next_state\n            episodes[idx_episodes, idx_time, 1] = reward_function(next_state)\n    return episodes\n\ndef estimate_state_values(*, discount, start_state_list, **kwargs):\n    state_values = {}\n    for start_state in start_state_list:\n        episodes = sample_episodes(start_state=start_state, **kwargs)\n        discounted_rewards = episodes[:, :, 1] * np.power(discount, np.arange(episodes.shape[1]))\n        returns = discounted_rewards.sum(axis=1)\n        value = returns.mean()\n        state_values[start_state] = value\n        print(f\"Value for state {start_state}: {value}\")\n    return state_values\n\nstate_values = estimate_state_values(discount=0.5,\n                                     start_state_list=[n_states - 1, 0, 1],\n                                     n_episodes=100, \n                                     length_episodes=10)\n\n\n# Compute the closed-form solution for the state values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "R = np.zeros(n_states)\nR[0] = 1\nnp.linalg.solve(np.eye(n_states) - 0.5 * transition_matrix, 0.5 * transition_matrix @ R)\n\n\n# ## MDP\n\n# Add action to move up/down and update the transition matrix accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_actions = 2  # up/down\nidx = np.arange(n_states)\ntransition_matrix = np.zeros((n_states, n_states, n_actions))\ntransition_matrix[idx, (idx - 1) % n_states, 0] = 0.1\ntransition_matrix[idx, (idx + 1) % n_states, 0] = 0.9\ntransition_matrix[idx, (idx - 1) % n_states, 1] = 0.9\ntransition_matrix[idx, (idx + 1) % n_states, 1] = 0.1\nprint(f\"normalised: {np.all(transition_matrix.sum(axis=1)==1)}\")\n\n\n# Adapt your sampling routine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def sample_episodes(n_episodes, length_episodes, start_state, policy):\n    episodes = np.zeros((n_episodes, length_episodes, 3), dtype=int)\n    episodes[:, 0, 0] = start_state\n    for idx_episodes in range(n_episodes):\n        for idx_time in range(1, length_episodes):\n            current_state = episodes[idx_episodes, idx_time - 1, 0]\n            action = np.random.choice(n_actions, p=policy[current_state])\n            probs = transition_matrix[current_state, :, action]\n            next_state = np.random.choice(n_states, p=probs)\n            episodes[idx_episodes, idx_time, 0] = next_state\n            episodes[idx_episodes, idx_time, 1] = reward_function(next_state)\n            episodes[idx_episodes, idx_time, 2] = action\n    return episodes\n\n\n# Define a uniform policy and estimate state values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "policy = np.zeros((n_states, n_actions))\npolicy[:, 0] = 0.5\npolicy[:, 1] = 0.5\n\nestimate_state_values(discount=0.5,\n                      start_state_list=[n_states - 1, 0, 1],\n                      n_episodes=100, \n                      length_episodes=10,\n                      policy=policy);\n\n\n# Change the policy to always go one step up and re-estimate the state values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "policy = np.zeros((n_states, n_actions))\npolicy[:, 0] = 1\npolicy[:, 1] = 0\n\nestimate_state_values(discount=0.5,\n                      start_state_list=[n_states - 1, 0, 1],\n                      n_episodes=100, \n                      length_episodes=10,\n                      policy=policy);\n\n\n# Experiment with different policies and try to improve them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "policy = np.zeros((n_states, n_actions))\npolicy[:int(n_states / 2), 0] = 0\npolicy[:int(n_states / 2), 1] = 1\npolicy[int(n_states / 2):, 0] = 1\npolicy[int(n_states / 2):, 1] = 0\n\nestimate_state_values(discount=0.5,\n                      start_state_list=[n_states - 1, 0, 1],\n                      n_episodes=100, \n                      length_episodes=10,\n                      policy=policy);"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}