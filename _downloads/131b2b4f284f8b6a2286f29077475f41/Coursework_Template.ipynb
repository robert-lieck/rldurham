{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Coursework Template\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# # Coursework Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dependencies and imports\n\n\n\nThis can take a minute...\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# !pip install swig\n# !pip install --upgrade rldurham"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport rldurham as rld"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reinforcement learning agent\n\n\n\nReplace this with your own agent, I recommend starting with TD3 (lecture 8).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Agent(torch.nn.Module):\n    def __init__(self):\n        super(Agent, self).__init__()\n\n    def sample_action(self, s):\n        return torch.rand(act_dim) * 2 - 1 # unifrom random in [-1, 1]\n\n    def put_data(self, action, observation, reward):\n        pass\n\n    def train(self):\n        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the environment and wrap it to capture statistics, logs, and videos\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\")\n# env = rld.make(\"rldurham/Walker\", render_mode=\"rgb_array\", hardcore=True) # only attempt this when your agent has solved the non-hardcore version\n\n# get statistics, logs, and videos\nenv = rld.Recorder(\n    env,\n    smoothing=10,                       # track rolling averages (useful for plotting)\n    video=True,                         # enable recording videos\n    video_folder=\"videos\",              # folder for videos\n    video_prefix=\"xxxx00-agent-video\",  # prefix for videos (replace xxxx00 with your username)\n    logs=True,                          # keep logs\n)\n\n# training on CPU recommended\nrld.check_device()\n\n# environment info\ndiscrete_act, discrete_obs, act_dim, obs_dim = rld.env_info(env, print_out=True)\n\n# render start image\nenv.reset(seed=42)\nrld.render(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# in the submission please use seed_everything with seed 42 for verification\nseed, observation, info = rld.seed_everything(42, env)\n\n# initialise agent\nagent = Agent()\nmax_episodes = 100\nmax_timesteps = 2000\n\n# track statistics for plotting\ntracker = rld.InfoTracker()\n\n# switch video recording off (only switch on every x episodes as this is slow)\nenv.video = False\n\n# training procedure\nfor episode in range(max_episodes):\n    \n    # recording statistics and video can be switched on and off (video recording is slow!)\n    # env.info = episode % 10 == 0   # track every x episodes (usually tracking every episode is fine)\n    # env.video = episode % 10 == 0  # record videos every x episodes (set BEFORE calling reset!)\n\n    # reset for new episode\n    observation, info = env.reset()\n\n    # run episode\n    for t in range(max_timesteps):\n        \n        # select the agent action\n        action = agent.sample_action(observation)\n\n        # take action in the environment\n        observation, reward, terminated, truncated, info = env.step(action)\n\n        # remember\n        agent.put_data(action, observation, reward)\n\n        # check whether done\n        done = terminated or truncated\n\n        # stop episode\n        if done:\n            break\n\n    # TRAIN THE AGENT HERE!\n            \n    # track and plot statistics\n    tracker.track(info)\n    if (episode + 1) % 10 == 0:\n        tracker.plot(r_mean_=True, r_std_=True, r_sum=dict(linestyle=':', marker='x'))\n\n# don't forget to close environment (e.g. triggers last video save)\nenv.close()\n\n# write log file (for coursework)\nenv.write_log(folder=\"logs\", file=\"xxxx00-agent-log.txt\")  # replace xxxx00 with your username\n\n\n# A small demo with a predefined heuristic that is suboptimal and has no notion of balance (and is designed for the orignal BipedalWalker environment)..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gymnasium.envs.box2d.bipedal_walker import BipedalWalkerHeuristics\n\nenv = rld.make(\n    \"rldurham/Walker\",\n    # \"BipedalWalker-v3\",\n    render_mode=\"human\",\n    # render_mode=\"rgb_array\",\n    hardcore=False,\n    # hardcore=True,\n)\n_, obs, info = rld.seed_everything(42, env)\n\nheuristics = BipedalWalkerHeuristics()\n\nact = heuristics.step_heuristic(obs)\nfor _ in range(500):\n    obs, rew, terminated, truncated, info = env.step(act)\n    act = heuristics.step_heuristic(obs)\n    if terminated or truncated:\n        break\n    if env.render_mode == \"rgb_array\":\n        rld.render(env, clear=True)\nenv.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}