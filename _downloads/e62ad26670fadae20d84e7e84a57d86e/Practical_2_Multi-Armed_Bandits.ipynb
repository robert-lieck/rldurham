{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Practical 2: Multi-Armed Bandits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# # Practical 2: Multi-Armed Bandits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\n\n# ## Basic Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Environment():\n    def __init__(self):\n        self.state = 0\n\n    def step(self, a):\n        self.state += 1\n        reward = -1\n        next_observation = 10 * self.state\n        return next_observation , reward\n        \n    def reset(self):\n        self.state = 0\n        return self.state\n    \n    def render(self):\n        print(self.state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = Environment()\nobs = env.reset()\nfor _ in range(10):\n    obs, rew = env.step(0)\n    env.render()\n\n\n# ## Multi-Armed Bandits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class BanditEnv():\n    def __init__(self):\n        self.state = 0\n        self.n_actions = 6\n        self.n_states = 0\n        self.probs = np.array([0.4, 0.2, 0.1, 0.1, 0.1, 0.7])\n        self.rewards = np.array([1.0, 0.1, 2.0, 0.5, 6.0, 70.])\n    \n    def step(self, a):\n        reward = -25\n        if np.random.uniform() < self.probs[a]:\n            reward += self.rewards[a]\n        return self.state, reward\n    \n    def reset(self):\n        self.state = 0\n        return self.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Agent():\n    def __init__(self, env):\n        self.env = env\n        \n    def sample_action(self, observation=0):\n        return np.random.randint(self.env.n_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = BanditEnv()\nagent = Agent(env)\no = env.reset()\nmoney = 0\nmoney_per_machine = np.zeros(env.n_actions)\nusage_per_machine = np.zeros(env.n_actions)\nfor episode in range(1000):\n    a = agent.sample_action(o)\n    o, r = env.step(a)\n    money += r\n    money_per_machine[a] += r\n    usage_per_machine[a] += 1\nprint(\"about \" + str(money))\nprint(\"about \" + str(money_per_machine/usage_per_machine))\n\n\n# ###  Solving Multi-Armed Bandits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Agent():\n\n    def __init__(self, env, alpha=None, epsilon=0):\n        self.env = env\n        self.alpha = alpha\n        self.epsilon = epsilon\n        self.k = np.zeros(self.env.n_actions)\n        self.q = np.zeros(self.env.n_actions)\n\n    @property\n    def q_corrected(self):\n        if self.alpha is None:\n            return self.q\n        else:\n            return self.q / (1 - (1 - self.alpha)**self.k + 1e-8)\n\n    def put_data(self, action, reward):\n        self.k[action] += 1\n        if self.alpha is None:\n            # exact average\n            if self.k[action] == 1:\n                self.q[action] = r\n            else:\n                self.q[action] += (r - self.q[action]) / self.k[action]\n        else:\n            # smoothing average\n            self.q[action] += self.alpha * (r - self.q[action])\n\n            self.q[action] = (1 - self.alpha) * self.q[action] + self.alpha * r\n        \n    def sample_action(self, state=0, epsilon=0.4):\n        if np.random.rand() < self.epsilon:\n            return np.random.randint(env.n_actions)\n        else:\n            return np.argmax(self.q_corrected)\n\nenv = BanditEnv()\nagent = Agent(env=env, alpha=0.1, epsilon=0.1)\ns = env.reset()\nfor episode in range(1000):\n    a = agent.sample_action(s)\n    s, r = env.step(a)\n    # learn to estimate the value of each action\n    agent.put_data(a, r)\n\nprint(agent.k)\nprint(agent.q)\nprint(agent.q_corrected)\n\n\n# ## Multi-Armed Multi-Room Bandits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class BanditEnv():\n    def __init__(self):\n        self.n_actions = 6+1\n        self.n_states  = 3\n        self.probs = np.array([\n            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n            [0.0, 0.4, 0.2, 0.1, 0.1, 0.1, 0.7],\n            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n        ])\n        self.rewards = np.array([\n            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ],\n            [0.0, 1.0, 0.1, 2.0, 0.5, 6.0, 70.0],\n            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ],\n        ])\n\n    def step(self, s, a):\n        r = -25\n\n        # if first action taken, move to next state with no reward\n        if a == 0:\n            return (s+1)%3, 0\n        # else pull the slot machine and get the reward, stay in current room\n        if np.random.uniform() < self.probs[s, a]:\n            r += self.rewards[s,a]\n\n        return s, r\n    \n    def reset(self):\n        return 0\n        \nclass Agent():\n    def __init__(self):\n        self.q = np.zeros([env.n_states, env.n_actions])\n\n    def sample_action(self, state=0, epsilon=0.4):\n        if np.random.rand() > epsilon:\n            return np.argmax(self.q[state,:])\n        else:\n            return np.random.randint(env.n_actions)\n\nenv = BanditEnv()\ns = env.reset()\nagent = Agent()\nfor episode in range(1000):\n    a = agent.sample_action(s)\n    next_s,r = env.step(s,a)\n    \n    agent.q[s,a] += 0.1 * (r - agent.q[s,a])\n    s = next_s\n    \nprint(agent.q)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}